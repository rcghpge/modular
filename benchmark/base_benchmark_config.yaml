##===----------------------------------------------------------------------===##
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##===----------------------------------------------------------------------===##

# Benchmark Configuration
# This configuration inherits from the base pipeline configuration and adds
# benchmark-specific overrides and settings that match benchmark_serving.py arguments.
#
# Usage (with MAXConfig classes):
#   from max.pipelines.lib import KVCacheConfig, SamplingConfig
#   
#   # Load benchmark-optimized configurations
#   kv_config = KVCacheConfig.from_config_file("base_benchmark_config.yaml")
#   sampling_config = SamplingConfig.from_config_file("base_benchmark_config.yaml")
#
# Usage (for benchmark parameters):
#   from benchmark_config import BenchmarkConfig
#   
#   # Using MAXConfig.from_config_file() interface (recommended)
#   benchmark_config = BenchmarkConfig.from_config_file("base_benchmark_config.yaml")
#   
#   # Or using the convenience function with overrides
#   from benchmark_config import load_benchmark_config
#   benchmark_config = load_benchmark_config("base_benchmark_config.yaml", {"model": "my-model"})

# Configuration metadata
name: "Base Benchmark Configuration"
description: "Base configuration for MAX benchmarking"
version: "0.0.1"

# TODO(zheng): This should depend on our base pipeline config. For now, we just
# disable it due to us not having wired up the path dependency yet.
# depends_on: null

# --------------- MAX Pipeline Config Overrides (for benchmarking only purposes) ------------------------------ #

# --------------- Benchmark Client Arguments (matching parse_args) ------------------------------ #

# Benchmark configuration that matches benchmark_serving.py's parse_args() function
benchmark_config:
  # Backend and API configuration
  backend: "modular"  # choices: vllm, vllm-chat, trt-llm, modular, modular-chat, sglang, sglang-chat
  base_url: null  # Server or API base url if not using host/port
  host: "localhost"
  port: 8000
  endpoint: "/v1/completions"  # /v1/completions, /v1/chat/completions, /v2/models/ensemble/generate_stream
  
  # Dataset configuration
  dataset_name: "sharegpt"  # Name of the dataset to benchmark on
  dataset_path: null  # Path to dataset file
  
  # Request configuration
  max_concurrency: 32  # Maximum concurrent requests (optimized for benchmarking)
  model: null  # Required - must be set when using config
  lora: null  # Optional LoRA name
  tokenizer: null  # Optional tokenizer (defaults to model if not specified)
  
  # Workload configuration
  num_prompts: 1000  # Number of prompts to process
  max_benchmark_duration_s: null  # Maximum benchmark duration in seconds
  num_chat_sessions: null  # Number of multiturn chat sessions
  delay_between_chat_turns: null  # Delay between chat turns in ms
  
  # Output control
  output_lengths: null  # Path to YAML file with output lengths or int
  max_output_len: 512  # Maximum output length per request (optimized for benchmarking)
  temperature: 0.0  # Temperature for sampling
  
  # Traffic control
  request_rate: 16.0  # Requests per second (finite rate for realistic benchmarking)
  burstiness: 1.0  # Burstiness factor (1.0 = Poisson process)
  ttft_skip_requests: 10  # Skip first N requests for TTFT measurements
  chat_warmup_delay_ms: 100.0  # Delay between starting chat sessions
  
  # Dataset-specific parameters
  sonnet_input_len: 550
  sonnet_prefix_len: 200
  arxiv_summarization_input_len: 15000
  random_input_len: 1024
  random_output_len: 128
  random_coefficient_of_variation: "0.3,0.7"
  random_image_size: ""
  random_sys_prompt_ratio: 0.0
  random_first_turn_ratio: 1.0
  random_max_num_unique_sys_prompt: 1
  random_distribution_type: "normal"  # choices: uniform, normal
  random_num_turns: 1
  
  # Control flags
  seed: 42  # Random seed for reproducibility
  trust_remote_code: false
  disable_tqdm: false
  skip_test_prompt: false
  collect_gpu_stats: true  # Enable GPU stats collection for benchmarks
  save_result: true  # Save benchmark results to JSON
  print_inputs_and_outputs: false
  
  # Result saving
  metadata: []  # Key-value pairs for metadata (format: ["key=value", ...])
  result_dir: "./benchmark_results"  # Directory to save results
  result_filename: null  # Custom filename (auto-generated if null)
  server_args: ""  # Server arguments string
  
  # File operations
  record_output_lengths: null  # File to save output lengths
