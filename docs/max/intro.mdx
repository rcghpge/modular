---
title: What is Modular
sidebar_label: Introduction
description: An overview of the Modular platform, what it does, and how to use it.
pagination_prev: null
pagination_next: null
---

import { Button } from '@mantine/core';
import DocLink from '@site/src/components/DocLink';
import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import ContactSection from '@site/src/components/ContactSection';


The Modular Platform is an open and fully-integrated suite of AI libraries and
tools that accelerates model serving and scales GenAI deployments. It abstracts
away hardware complexity so you can run the most popular open models with
industry-leading GPU and CPU performance without any code changes.

Our ready-to-deploy Docker container removes the complexity of deploying your
own GenAI endpoint. And unlike other serving solutions, Modular enables
customization across the entire stack. You can customize everything from the
serving pipeline and model architecture all the way down to the metal by
writing custom ops and GPU kernels in Mojo. Most importantly, Modular is
hardware-agnostic and free from vendor lock-in‚Äîno CUDA required‚Äîso your code
runs seamlessly across diverse systems.

It takes only a moment to start an OpenAI-compatible endpoint with a model from
Hugging Face:

<Tabs>
  <TabItem value="cli" label="CLI endpoint">

    ```sh
    max serve --model google/gemma-3-27b-it
    ```

  </TabItem>
  <TabItem value="docker" label="Docker endpoint">

    ```sh
    docker run --gpus=1 \
      -v ~/.cache/huggingface:/root/.cache/huggingface \
      -v ~/.cache/max_cache:/opt/venv/share/max/.max_cache \
      -p 8000:8000 \
      modular/max-nvidia-full:latest \
      --model-path google/gemma-3-27b-it
    ```
  </TabItem>
</Tabs>

```python
from openai import OpenAI

client = OpenAI(base_url="http://0.0.0.0:8000/v1", api_key="EMPTY")

completion = client.chat.completions.create(
    model="google/gemma-3-27b-it",
    messages=[
        {
            "role": "user",
            "content": "Write a one-sentence bedtime story about a unicorn.",
        },
    ],
)

print(completion.choices[0].message.content)
```

<Button component={DocLink} to='/max/get-started'>
  Try it now
</Button>

## Capabilities

- [x] **High-performance, portable serving**: Serve 500+ AI models from Hugging
Face using our OpenAI-compatible REST API with industry-leading performance
across GPUs and CPUs.

- [x] **Large-scale, GenAI deployment**: Scale massive GenAI inference services
across thousands of GPU nodes. Modular intelligently routes workloads across
models and hardware types to maximize throughput and minimize latency.

- [x] **Flexible, faster development**: Deploy with a single Docker container
that's under 1GB across multiple hardware types, compile in seconds rather than
hours, and develop faster with a slim toolchain that makes versioning and
dependency nightmares disappear.

- [x] **Customize everywhere**: Customize at any layer of the stack by writing
hardware-agnostic GPU and CPU kernels, porting models into Modular's optimized
graph format, or programming hardware directly with Mojo (no hardware-specific
libraries).

## Components

Modular is a vertically integrated AI infrastructure stack that spans from the
hardware all the way up to Kubernetes, and it provides entry points for users
at every level.

<figure>
  <img src={require('./images/modular-dev-stack.png').default}
       className="light" alt="" width="704" />
  <img src={require('./images/modular-dev-stack-dark.png').default}
       className="dark" alt="" width="704" />
  <figcaption><b>Figure 1.</b> A simplified diagram of how the Modular Platform
    scales your GenAI deployment.</figcaption>
</figure>

- ü¶£ **Mammoth**: A Kubernetes-native control plane, router, and
substrate specially-designed for large-scale distributed AI serving. It
supports multi-model management, prefill-aware routing, disaggregated compute
and cache, and other advanced AI optimizations.

- üßëüèª‚ÄçüöÄ **MAX**: A high-performance AI serving framework that includes advanced
model serving optimizations like speculative decoding, and graph compiler
optimizations like op-level fusions. It provides an OpenAI-compatible serving
endpoint, executes native MAX and PyTorch models across GPUs and CPUs, and can
be customized at the model and kernel level.

- üî• **Mojo**: A kernel-focused systems programming language that enables
high-performance GPU and CPU programming, blending Pythonic syntax with the
performance of C/C++ and the safety of Rust. All the kernels in MAX are written
with Mojo and it can be used to extend MAX Models with novel algorithms.

## Get started

You can create an OpenAI-compatible REST endpoint using our `max` CLI or our
Docker container:

- [**Start with pip**](/max/get-started): Install MAX with `pip` and run
inference with Python or a REST endpoint.

- [**Start with Docker**](/max/container): Run our Docker container to create a
REST endpoint.

In either case, you can select from hundreds of GenAI models in our [Model
repository](https://builds.modular.com/?category=models). You can also load
weights from Hugging Face or load your own fine-tuned weights.

For performance optimization, you can port models from PyTorch to MAX using the
[MAX Graph API](/max/develop/get-started-with-max-graph-in-python). For
deeper customization, you can extend MAX Models with [custom
operations](/max/develop/build-custom-ops) (ops) written in Mojo. Your custom
ops are automatically analyzed and fused into the model graph, delivering
low-level acceleration without sacrificing developer productivity.

:::note

Mammoth powers advanced routing and scaling capabilities behind the scenes for
Modular's Dedicated Endpoint and Enterprise
[editions](https://www.modular.com/pricing).

[Contact us](https://www.modular.com/request-demo) to learn how Mammoth
can help scale your workloads.

:::


## Stay in touch

<ContactSection />
