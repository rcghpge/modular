[project]
name = "bazel-pyproject"
version = "0"
requires-python = ">=3.10" # NOTE: Must be kept in sync with other global settings
dependencies = [
  "levenshtein",
  "absl-py",
  "aioboto3",
  "aiofiles",
  "aiohttp",
  "anyio>=4.12.1",
  "asgiref",
  "async-asgi-testclient",
  "boto3",
  "click>=8.1",
  "codeowners",
  "cyclopts",
  "datasets",
  "device-smi",
  "diffusers",
  "docutils",
  "einops",
  "einx",
  "exceptiongroup",
  "expandvars",
  "fastapi[standard]>=0.115.3", # >=0.115.3 is required for resolving CVE-2024-47874
  "fire ", # For mistral-evals.
  "filelock",
  "gguf",
  "hf-transfer",
  "httpx==0.27.2",
  "huggingface-hub[hf_xet]>=0.27.1",
  "hypothesis==6.108.4",
  "ipython",
  "jinja2",
  "jiwer",
  "kaleido==0.2.1",
  "kepler",
  "llguidance==1.3.0",
  "llvmlite",
  "libcst",
  "logbar",
  "markupsafe",
  "matplotlib",
  "msgspec",
  "munch",
  "mypy",
  "notebook",
  "numpy==2.2.6; python_version < '3.11'",
  "numpy>=2.3.0; python_version >= '3.11'",
  "nvitop",
  "onnxruntime", # Needed by faster_whisper
  "opentelemetry-api",
  "opentelemetry-exporter-otlp-proto-http>=1.28.2",
  "opentelemetry-exporter-prometheus",
  "opentelemetry-sdk",
  "packaging",
  "prometheus-client",
  "protobuf==6.33.5 ", # Must match bazel protobuf version
  "psutil>=5.9.0", # needed for setting `--timeout` for llvm-lit
  "py-cpuinfo",
  "pyarrow",
  "pydantic",
  "pydantic_core",
  "pydantic-settings==2.3.4",
  "pygame-ce",
  "pygments",
  "pyinstrument",
  "pytest-asyncio==0.23.7",
  "pytest-benchmark",
  "pytest-mock",
  "pytest-xdist",
  "pytest>=7.2",
  "python-json-logger",
  "PyYAML",
  "pyzmq",
  "plotext",
  "plotly",
  "requests>=2.28",
  "regex",
  "ruamel.yaml",
  "safetensors",
  "sentencepiece",
  "setuptools",
  "simpy",
  "sphinx==7.4.7",
  "sse-starlette",
  "sseclient-py",
  "starlette>=0.47.2", # transitively included by sse-starlette, >=0.47.2 is required for resolving CVE-2025-54121
  "taskgroup",
  "termcolor",
  "threadpoolctl",
  "tokenicer",
  "tomli",
  "tqdm",
  "transformers>=4.57",
  "types-aioboto3[essential]",
  "types-boto3[essential]",
  "typing-extensions",
  "uvloop>=0.21.0",
  "uvicorn",
  "wheel",

  # modeltool
  "google-auth==2.29.0",
  "google-cloud-bigquery==3.22.0",
  "pandas>=1.4.2",
  "requests>=2.28.0",
  "rich>=12.4.4",
  "scipy>=1.10.1",
  "tabulate>=0.8.9",

  # benchmarks
  "schema==0.7.5",

  # Blackwell external baseline benchmarks (flashinfer runtime deps)
  "apache-tvm-ffi>=0.1,<0.2",
  "ninja",  # JIT compilation at runtime

  # nsight-sql-tool
  "sqlalchemy>=2.0.29",

  #accuracy_testing_framework
  "ml_dtypes",

  # perfsect
  "responses>=0.23.3",

  # mblack
  "mypy-extensions>=0.4.3",
  "pathspec>=0.9.0",
  "platformdirs>=2",

  # pillow is required to get the torch tests configured correctly
  # in the wheel build.
  "pillow>=10.0.0",
  "locust",
  "openai==2.11.0",
  "librosa==0.10.2",
  "soundfile==0.12.1",
  "grpcio",
  "pytest-grpc",

  # mypy types
  "types-protobuf",
  "types-PyYAML",
  "types-setuptools",
  "types-tabulate",

  # tts
  "zhon",
  "zhconv",

  # Testing video processing for qwen2.5vl
  "opencv-python",
]

[dependency-groups]
amd = [
  "torch==2.9.0",
  "torchaudio",
  "torchvision==0.24.0",
  "triton==3.5.0; sys_platform == 'linux' and platform_machine == 'x86_64'",

  "gptqmodel; sys_platform == 'linux' and platform_machine == 'x86_64'",

  "accelerate",
  "lm-eval[api,ifeval,math]>=0.4.9.1",
  "mteb",
  "optimum",
  "peft", # For LoRA adapter support
  "sentence-transformers",
  "timm",
  "torchmetrics",
  "qwen-vl-utils",
]

cpu = [
  "torch==2.9.1",
  "torchaudio",
  "torchvision==0.24.1+cpu; sys_platform == 'linux' and platform_machine == 'x86_64'",
  "torchvision==0.24.1; platform_machine != 'x86_64'",
  "triton==3.5.1; sys_platform == 'linux' and platform_machine == 'x86_64'",

  "accelerate",
  "lm-eval[api,ifeval,math]>=0.4.9.1",
  "mteb",
  "optimum",
  "peft", # For LoRA adapter support
  "sentence-transformers",
  "timm",
  "torchmetrics",
  "qwen-vl-utils",
]

nvidia = [
  "torch==2.9.1",
  "torchaudio",
  "torchvision==0.24.1",
  "triton==3.5.1; sys_platform == 'linux' and platform_machine == 'x86_64'",

  "gptqmodel; sys_platform == 'linux' and platform_machine == 'x86_64'",
  "compressed-tensors==0.13.0; sys_platform == 'linux' and platform_machine == 'x86_64'",
  "vllm==0.14.1; sys_platform == 'linux' and platform_machine == 'x86_64' and python_version >= '3.11'",

  "accelerate",
  "flashinfer-cubin",
  "lm-eval[api,ifeval,math]>=0.4.9.1",
  "mteb",
  "optimum",
  "peft>=0.17.0", # For LoRA adapter support
  "sentence-transformers",
  "timm",
  "torchmetrics",
  "qwen-vl-utils",

  "nvidia-cudnn-frontend>=1.13.0; sys_platform == 'linux' and platform_machine == 'x86_64'",
  "nvidia-cutlass-dsl>=4.3.0; sys_platform == 'linux' and platform_machine == 'x86_64'",
]

[tool.uv]
default-groups = ["cpu"]
conflicts = [
  [
    { group = "amd" },
    { group = "cpu" },
    { group = "nvidia" },
  ],
]

environments = [
    "sys_platform == 'darwin'",
    "sys_platform == 'linux'",
]

override-dependencies = [
  "numba==0.63.1"
]

[tool.uv.sources]
torch = [
  { url = "https://repo.radeon.com/rocm/manylinux/rocm-rel-7.0.2/torch-2.9.1.dev20251204%2Brocm7.0.2.lw.git351ff442-cp313-cp313-linux_x86_64.whl" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { index = "pytorch-nvidia", group = "nvidia" },
]
torchvision = [
  { url = "https://repo.radeon.com/rocm/manylinux/rocm-rel-7.0.2/torchvision-0.24.0%2Brocm7.0.2.gitb919bd0c-cp313-cp313-linux_x86_64.whl" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { index = "pytorch-nvidia", group = "nvidia" },
]
torchaudio = [
  { url = "https://repo.radeon.com/rocm/manylinux/rocm-rel-7.0.2/torchaudio-2.9.0%2Brocm7.0.2.gite3c6ee2b-cp313-cp313-linux_x86_64.whl" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { index = "pytorch-nvidia", group = "nvidia" },
]
triton = [
  { url = "https://repo.radeon.com/rocm/manylinux/rocm-rel-7.0.2/triton-3.5.1%2Brocm7.0.2.gita272dfa8-cp313-cp313-linux_x86_64.whl" , group = "amd" },
  { index = "pytorch-cpu", group = "cpu" },
  { index = "pytorch-nvidia", group = "nvidia" },
]
gptqmodel = [
  { url = "https://github.com/ModelCloud/GPTQModel/releases/download/v2.0.0/gptqmodel-2.0.0+cu126torch2.6-cp313-cp313-linux_x86_64.whl" },
]

[[tool.uv.index]]
name = "pytorch-amd"
url = "https://download.pytorch.org/whl/rocm6.3"
explicit = true

[[tool.uv.index]]
name = "pytorch-cpu"
url = "https://download.pytorch.org/whl/cpu"
explicit = true

[[tool.uv.index]]
name = "pytorch-nvidia"
url = "https://download.pytorch.org/whl/cu128"
explicit = true
