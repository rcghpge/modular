# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #


from gpu import barrier
from gpu.host import DeviceContext
from gpu.id import thread_idx
from gpu.intrinsics import threadfence
from gpu.memory import AddressSpace
from gpu.mma import (
    WGMMADescriptor,
    wgmma_async,
    wgmma_commit_group_sync,
    wgmma_fence_aligned,
    wgmma_wait_group_sync,
)
from layout import IntTuple, Layout, LayoutTensor
from layout._utils import ManagedLayoutTensor


fn wgmma_f32_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    smem_operand_a_layout: Layout,
    smem_operand_b_layout: Layout,
    a_type: DType,
    b_type: DType,
](
    operand_a: LayoutTensor[a_type, Layout.row_major(M, K), MutableAnyOrigin],
    operand_b: LayoutTensor[b_type, Layout.row_major(K, N), MutableAnyOrigin],
    result_c: LayoutTensor[
        DType.float32, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var smem_operand_a = LayoutTensor[
        a_type,
        smem_operand_a_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var smem_operand_b = LayoutTensor[
        b_type,
        smem_operand_b_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.float32, 4](0)

    for k_i in range(K // WMMA_K):
        var operand_a_tile = operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_tile = operand_b.tile[WMMA_K, N](k_i, 0)
        var operand_a_sm_tile = smem_operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_sm_tile = smem_operand_b.tile[WMMA_K, N](k_i, 0)

        if thread_idx.x == 0:
            operand_a_sm_tile.copy_from(operand_a_tile)
            operand_b_sm_tile.copy_from(operand_b_tile)

        barrier()

        var mat_a_desc = WGMMADescriptor.create[8, 64](operand_a_sm_tile.ptr)
        var mat_b_desc = WGMMADescriptor.create[1, 8](operand_b_sm_tile.ptr)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type=a_type,
            b_type=b_type,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()
        threadfence()
        wgmma_fence_aligned()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    var th_local_res = (
        result_c.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0, 0][0] = c_reg[0]
    th_local_res[0, 0][1] = c_reg[1]
    th_local_res[1, 0][0] = c_reg[2]
    th_local_res[1, 0][1] = c_reg[3]


# CHECK-LABEL: wgmma_e4m3_e4m3_f32_64x8x32
# CHECK: 10440.0 10928.0 11385.0 11826.0 12366.0 12858.0 13315.0 13884.0
# CHECK: 10928.0 11464.0 11952.0 12409.0 12978.0 13518.0 14010.0 14595.0
# CHECK: 11385.0 11952.0 12487.0 12974.0 13558.0 14126.0 14665.0 15284.0
# CHECK: 11826.0 12409.0 12974.0 13507.0 14120.0 14702.0 15268.0 15933.0
# CHECK: 12366.0 12978.0 13558.0 14120.0 14794.0 15404.0 15983.0 16690.0
# CHECK: 12858.0 13518.0 14126.0 14702.0 15404.0 16074.0 16680.0 17398.0
# CHECK: 13315.0 14010.0 14665.0 15268.0 15983.0 16680.0 17344.0 18090.0
# CHECK: 13884.0 14595.0 15284.0 15933.0 16690.0 17398.0 18090.0 18908.0
# CHECK: 14420.0 15164.0 15868.0 16550.0 17352.0 18102.0 18804.0 19648.0
# CHECK: 14908.0 15700.0 16436.0 17132.0 17966.0 18760.0 19502.0 20356.0
# CHECK: 15365.0 16188.0 16970.0 17698.0 18544.0 19370.0 20154.0 21048.0
# CHECK: 15806.0 16644.0 17458.0 18230.0 19108.0 19944.0 20760.0 21694.0
# CHECK: 16346.0 17214.0 18042.0 18844.0 19782.0 20648.0 21474.0 22454.0
# CHECK: 16838.0 17754.0 18610.0 19426.0 20392.0 21318.0 22172.0 23162.0
# CHECK: 17294.0 18246.0 19148.0 19992.0 20970.0 21924.0 22836.0 23854.0
# CHECK: 17864.0 18830.0 19768.0 20656.0 21678.0 22642.0 23582.0 24672.0
# CHECK: 18400.0 19400.0 20352.0 21274.0 22340.0 23346.0 24296.0 25412.0
# CHECK: 18888.0 19936.0 20920.0 21856.0 22954.0 24004.0 24994.0 26120.0
# CHECK: 19344.0 20424.0 21456.0 22424.0 23536.0 24618.0 25652.0 26818.0
# CHECK: 19786.0 20880.0 21942.0 22956.0 24098.0 25192.0 26256.0 27464.0
# CHECK: 20324.0 21450.0 22524.0 23566.0 24768.0 25890.0 26964.0 28216.0
# CHECK: 20814.0 21988.0 23094.0 24148.0 25378.0 26560.0 27662.0 28924.0
# CHECK: 21272.0 22478.0 23632.0 24718.0 25960.0 27170.0 28332.0 29622.0
# CHECK: 21840.0 23064.0 24248.0 25380.0 26668.0 27888.0 29076.0 30440.0
# CHECK: 22376.0 23632.0 24832.0 25992.0 27324.0 28588.0 29784.0 31172.0
# CHECK: 22864.0 24168.0 25400.0 26576.0 27936.0 29244.0 30484.0 31880.0
# CHECK: 23320.0 24656.0 25936.0 27144.0 28520.0 29856.0 31140.0 32580.0
# CHECK: 23762.0 25112.0 26422.0 27676.0 29082.0 30432.0 31742.0 33224.0
# CHECK: 24300.0 25682.0 27004.0 28286.0 29752.0 31130.0 32452.0 33972.0
# CHECK: 24790.0 26220.0 27574.0 28868.0 30362.0 31800.0 33148.0 34684.0
# CHECK: 25248.0 26710.0 28112.0 29438.0 30944.0 32410.0 33820.0 35380.0
# CHECK: 25816.0 27296.0 28728.0 30100.0 31652.0 33128.0 34564.0 36200.0
# CHECK: 26352.0 27864.0 29312.0 30712.0 32308.0 33828.0 35272.0 36932.0
# CHECK: 26840.0 28400.0 29880.0 31296.0 32920.0 34484.0 35972.0 37640.0
# CHECK: 27296.0 28888.0 30416.0 31864.0 33504.0 35096.0 36628.0 38340.0
# CHECK: 27736.0 29344.0 30904.0 32400.0 34072.0 35680.0 37240.0 38996.0
# CHECK: 28144.0 29784.0 31356.0 32880.0 34596.0 36232.0 37804.0 39584.0
# CHECK: 28512.0 30192.0 31796.0 33332.0 35076.0 36756.0 38356.0 40148.0
# CHECK: 29112.0 30816.0 32460.0 34028.0 35816.0 37524.0 39168.0 41020.0
# CHECK: 29680.0 31416.0 33080.0 34684.0 36500.0 38248.0 39916.0 41808.0
# CHECK: 30208.0 31984.0 33680.0 35304.0 37156.0 38932.0 40640.0 42556.0
# CHECK: 30704.0 32512.0 34248.0 35904.0 37776.0 39588.0 41324.0 43280.0
# CHECK: 31176.0 33008.0 34776.0 36472.0 38376.0 40208.0 41980.0 43964.0
# CHECK: 31616.0 33480.0 35272.0 37000.0 38944.0 40808.0 42600.0 44620.0
# CHECK: 32016.0 33920.0 35740.0 37488.0 39460.0 41360.0 43180.0 45216.0
# CHECK: 32648.0 34576.0 36436.0 38212.0 40236.0 42164.0 44020.0 46116.0
# CHECK: 33248.0 35208.0 37092.0 38908.0 40960.0 42940.0 44824.0 46956.0
# CHECK: 33800.0 35808.0 37720.0 39556.0 41644.0 43648.0 45580.0 47736.0
# CHECK: 34320.0 36360.0 38320.0 40184.0 42292.0 44332.0 46288.0 48492.0
# CHECK: 34824.0 36880.0 38872.0 40784.0 42920.0 44980.0 46972.0 49200.0
# CHECK: 35296.0 37384.0 39392.0 41336.0 43520.0 45608.0 47620.0 49884.0
# CHECK: 35720.0 37856.0 39896.0 41856.0 44072.0 46208.0 48248.0 50532.0
# CHECK: 36116.0 38280.0 40364.0 42352.0 44580.0 46744.0 48828.0 51136.0
# CHECK: 36500.0 38676.0 40788.0 42820.0 45076.0 47252.0 49364.0 51716.0
# CHECK: 37104.0 39316.0 41440.0 43500.0 45832.0 48036.0 50160.0 52572.0
# CHECK: 37660.0 39920.0 42076.0 44144.0 46500.0 48776.0 50924.0 53344.0
# CHECK: 38184.0 40476.0 42680.0 44780.0 47144.0 49444.0 51664.0 54108.0
# CHECK: 38692.0 41000.0 43236.0 45384.0 47780.0 50088.0 52332.0 54848.0
# CHECK: 39168.0 41508.0 43760.0 45940.0 48384.0 50724.0 52976.0 55516.0
# CHECK: 39596.0 41984.0 44268.0 46464.0 48940.0 51328.0 53612.0 56160.0
# CHECK: 39996.0 42412.0 44740.0 46964.0 49452.0 51868.0 54196.0 56772.0
# CHECK: 40632.0 43068.0 45424.0 47692.0 50240.0 52668.0 55024.0 57676.0
# CHECK: 41232.0 43704.0 46080.0 48376.0 50968.0 53456.0 55824.0 58504.0
# CHECK: 41784.0 44304.0 46712.0 49024.0 51640.0 54168.0 56592.0 59280.0
def wgmma_e4m3_e4m3_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e4m3_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)
    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias wgmma_e4m3_e4m3_f32_kernel_fn = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e4m3fn,
    ]
    ctx.enqueue_function[wgmma_e4m3_e4m3_f32_kernel_fn](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e5m2_f32_64x8x32
# CHECK: 10500.0 10968.0 11421.0 11866.0 12266.0 12642.0 13239.0 13828.0
# CHECK: 10968.0 11524.0 11992.0 12445.0 12890.0 13290.0 13922.0 14519.0
# CHECK: 11421.0 11992.0 12547.0 13014.0 13466.0 13910.0 14565.0 15196.0
# CHECK: 11866.0 12445.0 13014.0 13567.0 14032.0 14482.0 15180.0 15833.0
# CHECK: 12266.0 12890.0 13466.0 14032.0 14582.0 15044.0 15747.0 16442.0
# CHECK: 12642.0 13290.0 13910.0 14482.0 15044.0 15590.0 16304.0 17002.0
# CHECK: 13239.0 13922.0 14565.0 15180.0 15747.0 16304.0 17164.0 17874.0
# CHECK: 13828.0 14519.0 15196.0 15833.0 16442.0 17002.0 17874.0 18728.0
# CHECK: 14368.0 15108.0 15792.0 16462.0 17092.0 17694.0 18568.0 19432.0
# CHECK: 14852.0 15648.0 16380.0 17056.0 17718.0 18340.0 19254.0 20120.0
# CHECK: 15320.0 16132.0 16920.0 17644.0 18312.0 18966.0 19900.0 20806.0
# CHECK: 15750.0 16600.0 17402.0 18180.0 18894.0 19552.0 20516.0 21440.0
# CHECK: 16148.0 17030.0 17868.0 18658.0 19424.0 20126.0 21092.0 22044.0
# CHECK: 16778.0 17684.0 18554.0 19380.0 20158.0 20912.0 21986.0 22940.0
# CHECK: 17392.0 18314.0 19208.0 20066.0 20880.0 21646.0 22772.0 23834.0
# CHECK: 17964.0 18928.0 19836.0 20716.0 21560.0 22360.0 23496.0 24608.0
# CHECK: 18488.0 19500.0 20448.0 21340.0 22204.0 23032.0 24200.0 25320.0
# CHECK: 18988.0 20024.0 21020.0 21952.0 22828.0 23676.0 24872.0 26024.0
# CHECK: 19440.0 20524.0 21544.0 22524.0 23440.0 24300.0 25516.0 26696.0
# CHECK: 19852.0 20976.0 22044.0 23048.0 24012.0 24912.0 26140.0 27340.0
# CHECK: 20248.0 21388.0 22492.0 23540.0 24524.0 25468.0 26732.0 27940.0
# CHECK: 20628.0 21784.0 22904.0 23988.0 25016.0 25980.0 27288.0 28532.0
# CHECK: 21256.0 22420.0 23556.0 24656.0 25720.0 26728.0 28120.0 29408.0
# CHECK: 21828.0 23048.0 24188.0 25300.0 26376.0 27416.0 28848.0 30216.0
# CHECK: 22344.0 23620.0 24816.0 25932.0 27020.0 28072.0 29536.0 30944.0
# CHECK: 22852.0 24136.0 25388.0 26560.0 27652.0 28716.0 30192.0 31632.0
# CHECK: 23320.0 24644.0 25904.0 27132.0 28280.0 29348.0 30836.0 32288.0
# CHECK: 23732.0 25112.0 26412.0 27648.0 28852.0 29976.0 31468.0 32932.0
# CHECK: 24124.0 25524.0 26876.0 28148.0 29356.0 30532.0 32076.0 33540.0
# CHECK: 24768.0 26172.0 27544.0 28868.0 30112.0 31292.0 32952.0 34468.0
# CHECK: 25384.0 26816.0 28192.0 29536.0 30832.0 32048.0 33712.0 35344.0
# CHECK: 25944.0 27432.0 28832.0 30176.0 31488.0 32752.0 34448.0 36080.0
# CHECK: 26464.0 27992.0 29448.0 30816.0 32128.0 33408.0 35152.0 36816.0
# CHECK: 26976.0 28512.0 30008.0 31432.0 32768.0 34048.0 35808.0 37520.0
# CHECK: 27432.0 29024.0 30528.0 31992.0 33384.0 34688.0 36448.0 38176.0
# CHECK: 27832.0 29480.0 31040.0 32512.0 33944.0 35304.0 37088.0 38816.0
# CHECK: 28224.0 29880.0 31496.0 33024.0 34464.0 35864.0 37704.0 39456.0
# CHECK: 28600.0 30272.0 31896.0 33480.0 34976.0 36384.0 38264.0 40072.0
# CHECK: 28968.0 30648.0 32280.0 33864.0 35408.0 36864.0 38744.0 40584.0
# CHECK: 29280.0 31016.0 32656.0 34248.0 35792.0 37296.0 39224.0 41064.0
# CHECK: 29568.0 31328.0 33024.0 34624.0 36176.0 37680.0 39656.0 41544.0
# CHECK: 29848.0 31616.0 33336.0 34992.0 36552.0 38064.0 40040.0 41976.0
# CHECK: 30600.0 32408.0 34136.0 35816.0 37432.0 38952.0 41064.0 43000.0
# CHECK: 31328.0 33160.0 34928.0 36616.0 38256.0 39832.0 41952.0 44024.0
# CHECK: 31968.0 33888.0 35680.0 37408.0 39056.0 40656.0 42832.0 44912.0
# CHECK: 32608.0 34528.0 36400.0 38144.0 39824.0 41424.0 43616.0 45744.0
# CHECK: 33224.0 35168.0 37040.0 38864.0 40560.0 42192.0 44384.0 46528.0
# CHECK: 33752.0 35784.0 37680.0 39504.0 41280.0 42928.0 45152.0 47296.0
# CHECK: 34272.0 36312.0 38296.0 40144.0 41920.0 43648.0 45888.0 48064.0
# CHECK: 34784.0 36832.0 38824.0 40760.0 42560.0 44288.0 46608.0 48800.0
# CHECK: 35272.0 37344.0 39344.0 41288.0 43176.0 44928.0 47248.0 49520.0
# CHECK: 35736.0 37832.0 39856.0 41808.0 43704.0 45544.0 47888.0 50160.0
# CHECK: 36128.0 38296.0 40344.0 42320.0 44224.0 46072.0 48504.0 50800.0
# CHECK: 36504.0 38688.0 40808.0 42808.0 44736.0 46592.0 49032.0 51416.0
# CHECK: 36872.0 39064.0 41192.0 43256.0 45200.0 47072.0 49512.0 51896.0
# CHECK: 37168.0 39432.0 41568.0 43640.0 45648.0 47536.0 49992.0 52376.0
# CHECK: 37456.0 39728.0 41936.0 44016.0 46032.0 47984.0 50456.0 52856.0
# CHECK: 38248.0 40528.0 42744.0 44896.0 46920.0 48880.0 51544.0 53960.0
# CHECK: 39032.0 41320.0 43544.0 45704.0 47800.0 49768.0 52440.0 55048.0
# CHECK: 39712.0 42104.0 44336.0 46504.0 48608.0 50648.0 53328.0 55944.0
# CHECK: 40352.0 42784.0 45120.0 47296.0 49408.0 51456.0 54208.0 56832.0
# CHECK: 40992.0 43424.0 45792.0 48064.0 50176.0 52224.0 54976.0 57664.0
# CHECK: 41568.0 44064.0 46432.0 48736.0 50944.0 52992.0 55744.0 58432.0
# CHECK: 42112.0 44640.0 47072.0 49376.0 51616.0 53760.0 56512.0 59200.0
def wgmma_e5m2_e5m2_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e5m2_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e5m2,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e4m3_e5m2_f32_64x8x32
# CHECK: 10460.0 10948.0 11405.0 11848.0 12258.0 12628.0 13231.0 13800.0
# CHECK: 10948.0 11484.0 11972.0 12429.0 12872.0 13282.0 13908.0 14511.0
# CHECK: 11404.0 11972.0 12507.0 12994.0 13450.0 13892.0 14557.0 15182.0
# CHECK: 11846.0 12428.0 12994.0 13527.0 14012.0 14466.0 15162.0 15825.0
# CHECK: 12384.0 12998.0 13577.0 14140.0 14670.0 15152.0 15891.0 16584.0
# CHECK: 12878.0 13536.0 14146.0 14721.0 15280.0 15806.0 16572.0 17306.0
# CHECK: 13332.0 14030.0 14683.0 15288.0 15858.0 16412.0 17220.0 17982.0
# CHECK: 13904.0 14612.0 15304.0 15951.0 16550.0 17114.0 17982.0 18784.0
# CHECK: 14452.0 15184.0 15885.0 16570.0 17210.0 17802.0 18678.0 19540.0
# CHECK: 14928.0 15732.0 16456.0 17148.0 17826.0 18458.0 19362.0 20230.0
# CHECK: 15381.0 16208.0 17002.0 17718.0 18402.0 19070.0 20012.0 20908.0
# CHECK: 15826.0 16660.0 17478.0 18262.0 18968.0 19642.0 20620.0 21552.0
# CHECK: 16358.0 17234.0 18058.0 18864.0 19638.0 20332.0 21346.0 22314.0
# CHECK: 16858.0 17766.0 18630.0 19442.0 20236.0 20998.0 22032.0 23034.0
# CHECK: 17310.0 18266.0 19160.0 20012.0 20810.0 21592.0 22692.0 23714.0
# CHECK: 17884.0 18846.0 19788.0 20668.0 21506.0 22290.0 23442.0 24528.0
# CHECK: 18440.0 19420.0 20368.0 21294.0 22160.0 22982.0 24136.0 25272.0
# CHECK: 18908.0 19976.0 20940.0 21872.0 22782.0 23632.0 24822.0 25960.0
# CHECK: 19360.0 20444.0 21496.0 22444.0 23360.0 24254.0 25472.0 26646.0
# CHECK: 19804.0 20896.0 21962.0 22996.0 23926.0 24824.0 26084.0 27284.0
# CHECK: 20332.0 21468.0 22540.0 23586.0 24600.0 25510.0 26804.0 28044.0
# CHECK: 20836.0 21996.0 23112.0 24164.0 25190.0 26184.0 27490.0 28764.0
# CHECK: 21288.0 22500.0 23640.0 24736.0 25768.0 26774.0 28164.0 29450.0
# CHECK: 21860.0 23080.0 24270.0 25388.0 26462.0 27472.0 28904.0 30272.0
# CHECK: 22416.0 23652.0 24848.0 26014.0 27108.0 28158.0 29592.0 31000.0
# CHECK: 22884.0 24208.0 25420.0 26592.0 27734.0 28804.0 30278.0 31688.0
# CHECK: 23336.0 24676.0 25976.0 27164.0 28312.0 29430.0 30924.0 32374.0
# CHECK: 23782.0 25128.0 26442.0 27716.0 28878.0 30000.0 31540.0 33008.0
# CHECK: 24308.0 25702.0 27020.0 28306.0 29552.0 30686.0 32260.0 33772.0
# CHECK: 24810.0 26228.0 27594.0 28884.0 30142.0 31360.0 32944.0 34492.0
# CHECK: 25264.0 26730.0 28120.0 29458.0 30720.0 31950.0 33620.0 35176.0
# CHECK: 25836.0 27312.0 28748.0 30108.0 31416.0 32648.0 34360.0 36000.0
# CHECK: 26392.0 27884.0 29328.0 30732.0 32060.0 33336.0 35048.0 36728.0
# CHECK: 26860.0 28440.0 29900.0 31312.0 32684.0 33980.0 35736.0 37416.0
# CHECK: 27312.0 28908.0 30456.0 31884.0 33264.0 34604.0 36380.0 38104.0
# CHECK: 27756.0 29360.0 30924.0 32440.0 33836.0 35184.0 37004.0 38748.0
# CHECK: 28152.0 29804.0 31372.0 32900.0 34380.0 35740.0 37564.0 39348.0
# CHECK: 28524.0 30200.0 31816.0 33348.0 34840.0 36284.0 38120.0 39908.0
# CHECK: 29120.0 30828.0 32468.0 34048.0 35544.0 37000.0 38984.0 40784.0
# CHECK: 29708.0 31424.0 33092.0 34692.0 36232.0 37688.0 39680.0 41624.0
# CHECK: 30248.0 32012.0 33688.0 35316.0 36876.0 38376.0 40368.0 42320.0
# CHECK: 30732.0 32552.0 34276.0 35912.0 37500.0 39020.0 41056.0 43008.0
# CHECK: 31200.0 33036.0 34816.0 36500.0 38096.0 39644.0 41700.0 43696.0
# CHECK: 31628.0 33504.0 35300.0 37040.0 38684.0 40240.0 42324.0 44340.0
# CHECK: 32024.0 33932.0 35764.0 37516.0 39212.0 40812.0 42900.0 44940.0
# CHECK: 32652.0 34584.0 36448.0 38236.0 39944.0 41596.0 43792.0 45836.0
# CHECK: 33264.0 35212.0 37100.0 38920.0 40664.0 42328.0 44576.0 46728.0
# CHECK: 33836.0 35824.0 37724.0 39564.0 41336.0 43032.0 45288.0 47488.0
# CHECK: 34360.0 36396.0 38336.0 40188.0 41980.0 43704.0 45992.0 48200.0
# CHECK: 34860.0 36920.0 38908.0 40800.0 42604.0 44348.0 46664.0 48904.0
# CHECK: 35312.0 37420.0 39432.0 41372.0 43216.0 44972.0 47308.0 49576.0
# CHECK: 35724.0 37872.0 39932.0 41896.0 43788.0 45584.0 47932.0 50220.0
# CHECK: 36120.0 38284.0 40380.0 42388.0 44300.0 46140.0 48524.0 50820.0
# CHECK: 36500.0 38680.0 40792.0 42836.0 44792.0 46652.0 49080.0 51412.0
# CHECK: 37128.0 39316.0 41444.0 43504.0 45496.0 47400.0 49912.0 52288.0
# CHECK: 37700.0 39944.0 42076.0 44148.0 46152.0 48088.0 50640.0 53096.0
# CHECK: 38216.0 40516.0 42704.0 44780.0 46796.0 48744.0 51328.0 53824.0
# CHECK: 38724.0 41032.0 43276.0 45408.0 47428.0 49388.0 51984.0 54512.0
# CHECK: 39192.0 41540.0 43792.0 45980.0 48056.0 50020.0 52628.0 55168.0
# CHECK: 39604.0 42008.0 44300.0 46496.0 48628.0 50648.0 53260.0 55812.0
# CHECK: 39996.0 42420.0 44764.0 46996.0 49132.0 51204.0 53868.0 56420.0
# CHECK: 40640.0 43068.0 45432.0 47716.0 49888.0 51964.0 54744.0 57348.0
# CHECK: 41256.0 43712.0 46080.0 48384.0 50608.0 52720.0 55504.0 58224.0
# CHECK: 41816.0 44328.0 46720.0 49024.0 51264.0 53424.0 56240.0 58960.0
def wgmma_e4m3_e5m2_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e5m2_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e5m2,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e4m3_f32_64x8x32
# CHECK: 10460.0 10948.0 11404.0 11846.0 12384.0 12878.0 13332.0 13904.0
# CHECK: 10948.0 11484.0 11972.0 12428.0 12998.0 13536.0 14030.0 14612.0
# CHECK: 11405.0 11972.0 12507.0 12994.0 13577.0 14146.0 14683.0 15304.0
# CHECK: 11848.0 12429.0 12994.0 13527.0 14140.0 14721.0 15288.0 15951.0
# CHECK: 12258.0 12872.0 13450.0 14012.0 14670.0 15280.0 15858.0 16550.0
# CHECK: 12628.0 13282.0 13892.0 14466.0 15152.0 15806.0 16412.0 17114.0
# CHECK: 13231.0 13908.0 14557.0 15162.0 15891.0 16572.0 17220.0 17982.0
# CHECK: 13800.0 14511.0 15182.0 15825.0 16584.0 17306.0 17982.0 18784.0
# CHECK: 14328.0 15080.0 15784.0 16448.0 17244.0 17996.0 18712.0 19540.0
# CHECK: 14824.0 15608.0 16352.0 17048.0 17864.0 18652.0 19396.0 20264.0
# CHECK: 15296.0 16104.0 16880.0 17616.0 18464.0 19272.0 20052.0 20948.0
# CHECK: 15738.0 16576.0 17374.0 18140.0 19026.0 19864.0 20662.0 21592.0
# CHECK: 16140.0 17018.0 17844.0 18630.0 19544.0 20418.0 21244.0 22190.0
# CHECK: 16774.0 17676.0 18542.0 19356.0 20322.0 21224.0 22086.0 23092.0
# CHECK: 17376.0 18310.0 19200.0 20054.0 21048.0 22002.0 22892.0 23934.0
# CHECK: 17928.0 18912.0 19832.0 20708.0 21740.0 22720.0 23660.0 24728.0
# CHECK: 18448.0 19464.0 20432.0 21336.0 22388.0 23404.0 24368.0 25484.0
# CHECK: 18952.0 19984.0 20984.0 21936.0 23016.0 24052.0 25052.0 26192.0
# CHECK: 19424.0 20488.0 21504.0 22488.0 23616.0 24680.0 25700.0 26876.0
# CHECK: 19848.0 20960.0 22008.0 23008.0 24168.0 25280.0 26328.0 27524.0
# CHECK: 20244.0 21384.0 22476.0 23504.0 24676.0 25816.0 26908.0 28128.0
# CHECK: 20628.0 21780.0 22900.0 23972.0 25172.0 26324.0 27444.0 28708.0
# CHECK: 21232.0 22420.0 23552.0 24652.0 25928.0 27108.0 28240.0 29564.0
# CHECK: 21788.0 23024.0 24188.0 25296.0 26596.0 27848.0 29004.0 30336.0
# CHECK: 22312.0 23580.0 24792.0 25932.0 27240.0 28516.0 29744.0 31100.0
# CHECK: 22820.0 24104.0 25348.0 26536.0 27876.0 29160.0 30412.0 31840.0
# CHECK: 23296.0 24612.0 25872.0 27092.0 28480.0 29796.0 31056.0 32508.0
# CHECK: 23724.0 25088.0 26380.0 27616.0 29036.0 30400.0 31692.0 33152.0
# CHECK: 24124.0 25516.0 26852.0 28116.0 29548.0 30940.0 32276.0 33764.0
# CHECK: 24760.0 26172.0 27536.0 28844.0 30336.0 31740.0 33104.0 34668.0
# CHECK: 25360.0 26808.0 28192.0 29528.0 31064.0 32528.0 33904.0 35496.0
# CHECK: 25912.0 27408.0 28824.0 30176.0 31736.0 33240.0 34672.0 36272.0
# CHECK: 26432.0 27960.0 29424.0 30808.0 32384.0 33912.0 35384.0 37040.0
# CHECK: 26936.0 28480.0 29976.0 31408.0 33016.0 34560.0 36056.0 37752.0
# CHECK: 27408.0 28984.0 30496.0 31960.0 33616.0 35192.0 36704.0 38424.0
# CHECK: 27832.0 29456.0 31000.0 32480.0 34168.0 35792.0 37336.0 39072.0
# CHECK: 28224.0 29880.0 31472.0 32984.0 34688.0 36344.0 37936.0 39704.0
# CHECK: 28600.0 30272.0 31896.0 33456.0 35192.0 36864.0 38488.0 40304.0
# CHECK: 28944.0 30648.0 32280.0 33864.0 35640.0 37336.0 38968.0 40808.0
# CHECK: 29256.0 30992.0 32656.0 34248.0 36048.0 37784.0 39440.0 41288.0
# CHECK: 29552.0 31304.0 33000.0 34624.0 36432.0 38192.0 39888.0 41760.0
# CHECK: 29832.0 31600.0 33312.0 34968.0 36808.0 38576.0 40296.0 42208.0
# CHECK: 30592.0 32392.0 34120.0 35792.0 37728.0 39528.0 41256.0 43256.0
# CHECK: 31288.0 33152.0 34912.0 36600.0 38552.0 40448.0 42208.0 44216.0
# CHECK: 31936.0 33848.0 35672.0 37392.0 39360.0 41272.0 43128.0 45168.0
# CHECK: 32568.0 34496.0 36360.0 38136.0 40128.0 42048.0 43912.0 46040.0
# CHECK: 33184.0 35128.0 37008.0 38824.0 40872.0 42816.0 44688.0 46824.0
# CHECK: 33752.0 35744.0 37640.0 39472.0 41560.0 43560.0 45456.0 47600.0
# CHECK: 34272.0 36312.0 38256.0 40104.0 42208.0 44248.0 46200.0 48368.0
# CHECK: 34776.0 36832.0 38824.0 40720.0 42840.0 44896.0 46888.0 49112.0
# CHECK: 35264.0 37336.0 39344.0 41288.0 43456.0 45528.0 47536.0 49800.0
# CHECK: 35704.0 37824.0 39848.0 41808.0 44024.0 46144.0 48168.0 50448.0
# CHECK: 36096.0 38264.0 40336.0 42312.0 44544.0 46712.0 48784.0 51080.0
# CHECK: 36472.0 38656.0 40776.0 42800.0 45048.0 47232.0 49352.0 51696.0
# CHECK: 36840.0 39032.0 41160.0 43224.0 45512.0 47704.0 49832.0 52216.0
# CHECK: 37168.0 39400.0 41536.0 43608.0 45936.0 48168.0 50304.0 52696.0
# CHECK: 37456.0 39728.0 41904.0 43984.0 46320.0 48592.0 50768.0 53168.0
# CHECK: 38248.0 40528.0 42744.0 44864.0 47272.0 49552.0 51768.0 54272.0
# CHECK: 38984.0 41320.0 43544.0 45704.0 48152.0 50504.0 52728.0 55272.0
# CHECK: 39664.0 42056.0 44336.0 46504.0 48992.0 51384.0 53680.0 56232.0
# CHECK: 40320.0 42736.0 45072.0 47296.0 49792.0 52224.0 54560.0 57184.0
# CHECK: 40960.0 43392.0 45744.0 48016.0 50560.0 52992.0 55360.0 58016.0
# CHECK: 41552.0 44032.0 46400.0 48688.0 51280.0 53760.0 56128.0 58816.0
# CHECK: 42096.0 44624.0 47040.0 49344.0 51952.0 54480.0 56896.0 59584.0
def wgmma_e5m2_e4m3_f32_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e4m3_f32_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float32,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f32_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e4m3fn,
    ]
    ctx.enqueue_function[kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


def main():
    with DeviceContext() as ctx:
        wgmma_e4m3_e4m3_f32_64x8x32(ctx)
        wgmma_e5m2_e5m2_f32_64x8x32(ctx)
        wgmma_e4m3_e5m2_f32_64x8x32(ctx)
        wgmma_e5m2_e4m3_f32_64x8x32(ctx)
