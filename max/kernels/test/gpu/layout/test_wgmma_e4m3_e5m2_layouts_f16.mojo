# ===----------------------------------------------------------------------=== #
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #


from gpu import barrier
from gpu.host import DeviceContext
from gpu import thread_idx
from gpu.intrinsics import threadfence
from gpu.mma import (
    WGMMADescriptor,
    wgmma_async,
    wgmma_commit_group_sync,
    wgmma_fence_aligned,
    wgmma_wait_group_sync,
)
from layout import IntTuple, Layout, LayoutTensor
from layout._utils import ManagedLayoutTensor
from memory import bitcast


fn wgmma_f16_kernel[
    M: Int,
    N: Int,
    K: Int,
    WMMA_M: Int,
    WMMA_N: Int,
    WMMA_K: Int,
    smem_operand_a_layout: Layout,
    smem_operand_b_layout: Layout,
    a_type: DType,
    b_type: DType,
](
    operand_a: LayoutTensor[a_type, Layout.row_major(M, K), MutableAnyOrigin],
    operand_b: LayoutTensor[b_type, Layout.row_major(K, N), MutableAnyOrigin],
    result_c: LayoutTensor[
        DType.float16, Layout.row_major(M, N), MutableAnyOrigin
    ],
):
    var smem_operand_a = LayoutTensor[
        a_type,
        smem_operand_a_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var smem_operand_b = LayoutTensor[
        b_type,
        smem_operand_b_layout,
        MutableAnyOrigin,
        address_space = AddressSpace.SHARED,
    ].stack_allocation()

    var c_reg = SIMD[DType.uint32, 2](0)

    for k_i in range(K // WMMA_K):
        var operand_a_tile = operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_tile = operand_b.tile[WMMA_K, N](k_i, 0)
        var operand_a_sm_tile = smem_operand_a.tile[M, WMMA_K](0, k_i)
        var operand_b_sm_tile = smem_operand_b.tile[WMMA_K, N](k_i, 0)

        if thread_idx.x == 0:
            operand_a_sm_tile.copy_from(operand_a_tile)
            operand_b_sm_tile.copy_from(operand_b_tile)

        barrier()

        var mat_a_desc = WGMMADescriptor.create[8, 64](operand_a_sm_tile.ptr)
        var mat_b_desc = WGMMADescriptor.create[1, 8](operand_b_sm_tile.ptr)

        wgmma_fence_aligned()

        c_reg = wgmma_async[
            WMMA_M,
            WMMA_N,
            WMMA_K,
            a_type=a_type,
            b_type=b_type,
            accum_type = DType.float16,
        ](mat_a_desc, mat_b_desc, c_reg)
        wgmma_commit_group_sync()
        wgmma_wait_group_sync()
        threadfence()
        wgmma_fence_aligned()

    var warp_id = thread_idx.x // 32
    var lan_id = thread_idx.x % 32
    # Refer to this layout:
    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-D.png
    # Each warp updates a 16x8 tile, and within each tile,
    # every thread updates a 1x2 vector. The resulting distribution layout
    # is as follows:
    c0 = bitcast[DType.float16, 4](c_reg)
    var th_local_res = (
        result_c.tile[16, 8](warp_id, 0)
        .vectorize[1, 2]()
        .distribute[Layout.row_major(8, 4)](lan_id)
    )
    th_local_res[0, 0][0] = c0[0]
    th_local_res[0, 0][1] = c0[1]
    th_local_res[1, 0][0] = c0[2]
    th_local_res[1, 0][1] = c0[3]


# CHECK-LABEL: wgmma_e4m3_e4m3_f16_64x8x32
# CHECK: 10440.0 10928.0 11384.0 11824.0 12368.0 12856.0 13312.0 13888.0
# CHECK: 10928.0 11464.0 11952.0 12408.0 12976.0 13520.0 14008.0 14592.0
# CHECK: 11384.0 11952.0 12488.0 12976.0 13560.0 14128.0 14664.0 15280.0
# CHECK: 11824.0 12408.0 12976.0 13504.0 14120.0 14704.0 15264.0 15936.0
# CHECK: 12368.0 12976.0 13560.0 14120.0 14792.0 15408.0 15984.0 16688.0
# CHECK: 12856.0 13520.0 14128.0 14704.0 15408.0 16072.0 16672.0 17392.0
# CHECK: 13312.0 14008.0 14664.0 15264.0 15984.0 16672.0 17344.0 18096.0
# CHECK: 13888.0 14592.0 15280.0 15936.0 16688.0 17392.0 18096.0 18912.0
# CHECK: 14416.0 15168.0 15872.0 16544.0 17344.0 18096.0 18800.0 19648.0
# CHECK: 14912.0 15696.0 16432.0 17136.0 17968.0 18752.0 19504.0 20352.0
# CHECK: 15368.0 16192.0 16976.0 17696.0 18544.0 19376.0 20160.0 21056.0
# CHECK: 15808.0 16640.0 17456.0 18224.0 19104.0 19952.0 20768.0 21696.0
# CHECK: 16344.0 17216.0 18048.0 18848.0 19776.0 20640.0 21472.0 22448.0
# CHECK: 16832.0 17760.0 18608.0 19424.0 20384.0 21312.0 22176.0 23168.0
# CHECK: 17296.0 18240.0 19152.0 20000.0 20976.0 21920.0 22832.0 23856.0
# CHECK: 17856.0 18832.0 19776.0 20656.0 21680.0 22640.0 23584.0 24672.0
# CHECK: 18400.0 19392.0 20352.0 21280.0 22336.0 23344.0 24288.0 25408.0
# CHECK: 18880.0 19936.0 20928.0 21856.0 22960.0 24000.0 24992.0 26112.0
# CHECK: 19344.0 20416.0 21456.0 22432.0 23536.0 24624.0 25648.0 26816.0
# CHECK: 19792.0 20880.0 21936.0 22960.0 24096.0 25184.0 26256.0 27456.0
# CHECK: 20320.0 21456.0 22528.0 23568.0 24768.0 25888.0 26960.0 28224.0
# CHECK: 20816.0 21984.0 23088.0 24144.0 25376.0 26560.0 27664.0 28928.0
# CHECK: 21280.0 22480.0 23632.0 24720.0 25952.0 27168.0 28336.0 29616.0
# CHECK: 21840.0 23072.0 24256.0 25376.0 26672.0 27888.0 29072.0 30432.0
# CHECK: 22368.0 23632.0 24832.0 25984.0 27328.0 28592.0 29792.0 31168.0
# CHECK: 22864.0 24160.0 25408.0 26576.0 27936.0 29248.0 30480.0 31872.0
# CHECK: 23328.0 24656.0 25936.0 27136.0 28512.0 29856.0 31136.0 32576.0
# CHECK: 23760.0 25120.0 26416.0 27680.0 29088.0 30432.0 31744.0 33216.0
# CHECK: 24304.0 25680.0 27008.0 28288.0 29760.0 31136.0 32448.0 33984.0
# CHECK: 24784.0 26224.0 27568.0 28864.0 30368.0 31808.0 33152.0 34688.0
# CHECK: 25248.0 26704.0 28112.0 29440.0 30944.0 32416.0 33824.0 35392.0
# CHECK: 25824.0 27296.0 28736.0 30096.0 31648.0 33120.0 34560.0 36192.0
# CHECK: 26352.0 27872.0 29312.0 30720.0 32304.0 33824.0 35264.0 36928.0
# CHECK: 26848.0 28400.0 29888.0 31296.0 32928.0 34496.0 35968.0 37632.0
# CHECK: 27296.0 28896.0 30416.0 31872.0 33504.0 35104.0 36640.0 38336.0
# CHECK: 27744.0 29344.0 30912.0 32400.0 34080.0 35680.0 37248.0 39008.0
# CHECK: 28144.0 29792.0 31360.0 32896.0 34592.0 36224.0 37792.0 39584.0
# CHECK: 28512.0 30192.0 31792.0 33344.0 35072.0 36768.0 38368.0 40160.0
# CHECK: 29120.0 30816.0 32464.0 34016.0 35808.0 37536.0 39168.0 41024.0
# CHECK: 29680.0 31424.0 33088.0 34688.0 36512.0 38240.0 39904.0 41792.0
# CHECK: 30208.0 31984.0 33664.0 35296.0 37152.0 38944.0 40640.0 42560.0
# CHECK: 30704.0 32512.0 34240.0 35904.0 37760.0 39584.0 41312.0 43264.0
# CHECK: 31168.0 33024.0 34784.0 36480.0 38368.0 40192.0 41984.0 43968.0
# CHECK: 31616.0 33472.0 35264.0 36992.0 38944.0 40800.0 42592.0 44608.0
# CHECK: 32016.0 33920.0 35744.0 37504.0 39456.0 41344.0 43168.0 45216.0
# CHECK: 32640.0 34560.0 36448.0 38208.0 40224.0 42176.0 44032.0 46112.0
# CHECK: 33248.0 35200.0 37088.0 38912.0 40960.0 42944.0 44832.0 46944.0
# CHECK: 33792.0 35808.0 37728.0 39552.0 41632.0 43648.0 45568.0 47744.0
# CHECK: 34304.0 36352.0 38336.0 40192.0 42304.0 44320.0 46272.0 48480.0
# CHECK: 34816.0 36864.0 38880.0 40768.0 42912.0 44992.0 46976.0 49216.0
# CHECK: 35296.0 37376.0 39392.0 41344.0 43520.0 45600.0 47616.0 49888.0
# CHECK: 35712.0 37856.0 39904.0 41856.0 44064.0 46208.0 48256.0 50528.0
# CHECK: 36128.0 38272.0 40352.0 42368.0 44576.0 46752.0 48832.0 51136.0
# CHECK: 36512.0 38688.0 40800.0 42816.0 45088.0 47264.0 49376.0 51712.0
# CHECK: 37120.0 39328.0 41440.0 43488.0 45824.0 48032.0 50176.0 52576.0
# CHECK: 37664.0 39936.0 42080.0 44160.0 46496.0 48768.0 50912.0 53344.0
# CHECK: 38176.0 40480.0 42688.0 44768.0 47136.0 49440.0 51648.0 54112.0
# CHECK: 38688.0 40992.0 43232.0 45376.0 47776.0 50080.0 52320.0 54848.0
# CHECK: 39168.0 41504.0 43776.0 45952.0 48384.0 50720.0 52992.0 55520.0
# CHECK: 39584.0 41984.0 44256.0 46464.0 48928.0 51328.0 53600.0 56160.0
# CHECK: 40000.0 42400.0 44736.0 46976.0 49440.0 51872.0 54208.0 56768.0
# CHECK: 40640.0 43072.0 45440.0 47680.0 50240.0 52672.0 55040.0 57664.0
# CHECK: 41216.0 43712.0 46080.0 48384.0 50976.0 53440.0 55808.0 58496.0
# CHECK: 41792.0 44288.0 46720.0 49024.0 51648.0 54176.0 56576.0 59264.0
def wgmma_e4m3_e4m3_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e4m3_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e4m3fn,
    ]
    ctx.enqueue_function_checked[kernel, kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e5m2_f16_64x8x32
# CHECK: 10496.0 10968.0 11424.0 11864.0 12264.0 12640.0 13240.0 13824.0
# CHECK: 10968.0 11520.0 11992.0 12448.0 12888.0 13288.0 13920.0 14520.0
# CHECK: 11424.0 11992.0 12544.0 13016.0 13464.0 13912.0 14568.0 15200.0
# CHECK: 11864.0 12448.0 13016.0 13568.0 14032.0 14480.0 15184.0 15832.0
# CHECK: 12264.0 12888.0 13464.0 14032.0 14584.0 15040.0 15744.0 16448.0
# CHECK: 12640.0 13288.0 13912.0 14480.0 15040.0 15592.0 16304.0 17008.0
# CHECK: 13240.0 13920.0 14568.0 15184.0 15744.0 16304.0 17168.0 17872.0
# CHECK: 13824.0 14520.0 15200.0 15832.0 16448.0 17008.0 17872.0 18736.0
# CHECK: 14368.0 15104.0 15792.0 16464.0 17088.0 17696.0 18560.0 19424.0
# CHECK: 14848.0 15648.0 16384.0 17056.0 17712.0 18336.0 19248.0 20128.0
# CHECK: 15320.0 16128.0 16928.0 17648.0 18304.0 18960.0 19904.0 20800.0
# CHECK: 15752.0 16608.0 17408.0 18176.0 18896.0 19552.0 20512.0 21440.0
# CHECK: 16144.0 17024.0 17872.0 18656.0 19424.0 20128.0 21088.0 22048.0
# CHECK: 16784.0 17680.0 18560.0 19376.0 20160.0 20912.0 21984.0 22944.0
# CHECK: 17392.0 18320.0 19200.0 20064.0 20880.0 21648.0 22768.0 23840.0
# CHECK: 17968.0 18928.0 19840.0 20720.0 21568.0 22368.0 23488.0 24608.0
# CHECK: 18496.0 19504.0 20448.0 21344.0 22208.0 23040.0 24192.0 25312.0
# CHECK: 18992.0 20032.0 21024.0 21952.0 22832.0 23680.0 24864.0 26016.0
# CHECK: 19440.0 20528.0 21536.0 22528.0 23440.0 24304.0 25520.0 26688.0
# CHECK: 19856.0 20976.0 22048.0 23040.0 24016.0 24912.0 26144.0 27344.0
# CHECK: 20256.0 21392.0 22496.0 23536.0 24528.0 25472.0 26736.0 27936.0
# CHECK: 20624.0 21792.0 22912.0 23984.0 25024.0 25984.0 27296.0 28528.0
# CHECK: 21248.0 22416.0 23552.0 24656.0 25728.0 26720.0 28128.0 29408.0
# CHECK: 21824.0 23040.0 24192.0 25296.0 26368.0 27424.0 28848.0 30208.0
# CHECK: 22336.0 23616.0 24816.0 25936.0 27024.0 28064.0 29536.0 30944.0
# CHECK: 22848.0 24128.0 25392.0 26560.0 27648.0 28720.0 30192.0 31632.0
# CHECK: 23328.0 24640.0 25904.0 27136.0 28288.0 29344.0 30832.0 32288.0
# CHECK: 23728.0 25120.0 26416.0 27648.0 28848.0 29984.0 31472.0 32928.0
# CHECK: 24128.0 25520.0 26880.0 28144.0 29360.0 30528.0 32080.0 33536.0
# CHECK: 24768.0 26176.0 27552.0 28864.0 30112.0 31296.0 32960.0 34464.0
# CHECK: 25376.0 26816.0 28192.0 29536.0 30832.0 32048.0 33728.0 35328.0
# CHECK: 25952.0 27424.0 28832.0 30176.0 31488.0 32752.0 34432.0 36096.0
# CHECK: 26464.0 28000.0 29440.0 30816.0 32128.0 33408.0 35136.0 36800.0
# CHECK: 26976.0 28512.0 30016.0 31424.0 32768.0 34048.0 35808.0 37504.0
# CHECK: 27424.0 29024.0 30528.0 32000.0 33376.0 34688.0 36448.0 38176.0
# CHECK: 27840.0 29472.0 31040.0 32512.0 33952.0 35296.0 37088.0 38816.0
# CHECK: 28224.0 29888.0 31488.0 33024.0 34464.0 35872.0 37696.0 39456.0
# CHECK: 28608.0 30272.0 31904.0 33472.0 34976.0 36384.0 38272.0 40064.0
# CHECK: 28960.0 30656.0 32288.0 33856.0 35392.0 36864.0 38752.0 40576.0
# CHECK: 29280.0 31008.0 32656.0 34240.0 35776.0 37312.0 39232.0 41056.0
# CHECK: 29568.0 31328.0 33024.0 34624.0 36160.0 37696.0 39648.0 41536.0
# CHECK: 29856.0 31616.0 33344.0 35008.0 36544.0 38080.0 40032.0 41984.0
# CHECK: 30592.0 32416.0 34144.0 35808.0 37440.0 38944.0 41056.0 43008.0
# CHECK: 31328.0 33152.0 34944.0 36608.0 38272.0 39840.0 41952.0 44032.0
# CHECK: 31968.0 33888.0 35680.0 37408.0 39040.0 40640.0 42816.0 44928.0
# CHECK: 32608.0 34528.0 36416.0 38144.0 39808.0 41408.0 43616.0 45760.0
# CHECK: 33216.0 35168.0 37056.0 38848.0 40576.0 42176.0 44384.0 46528.0
# CHECK: 33760.0 35776.0 37696.0 39488.0 41280.0 42944.0 45152.0 47296.0
# CHECK: 34272.0 36320.0 38304.0 40128.0 41920.0 43648.0 45888.0 48064.0
# CHECK: 34784.0 36832.0 38816.0 40768.0 42560.0 44288.0 46592.0 48800.0
# CHECK: 35264.0 37344.0 39360.0 41280.0 43168.0 44928.0 47232.0 49536.0
# CHECK: 35744.0 37824.0 39872.0 41792.0 43712.0 45536.0 47872.0 50176.0
# CHECK: 36128.0 38304.0 40352.0 42304.0 44224.0 46080.0 48512.0 50816.0
# CHECK: 36512.0 38688.0 40800.0 42816.0 44736.0 46592.0 49024.0 51424.0
# CHECK: 36864.0 39072.0 41184.0 43264.0 45184.0 47072.0 49504.0 51904.0
# CHECK: 37184.0 39424.0 41568.0 43648.0 45632.0 47552.0 49984.0 52384.0
# CHECK: 37440.0 39744.0 41920.0 44032.0 46016.0 48000.0 50464.0 52864.0
# CHECK: 38240.0 40512.0 42752.0 44896.0 46912.0 48896.0 51552.0 53952.0
# CHECK: 39040.0 41312.0 43552.0 45696.0 47808.0 49760.0 52448.0 55040.0
# CHECK: 39712.0 42112.0 44352.0 46496.0 48608.0 50656.0 53312.0 55936.0
# CHECK: 40352.0 42784.0 45120.0 47296.0 49408.0 51456.0 54208.0 56832.0
# CHECK: 40992.0 43424.0 45792.0 48064.0 50176.0 52224.0 54976.0 57664.0
# CHECK: 41568.0 44064.0 46432.0 48736.0 50944.0 52992.0 55744.0 58432.0
# CHECK: 42112.0 44640.0 47072.0 49376.0 51616.0 53760.0 56512.0 59200.0
def wgmma_e5m2_e5m2_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e5m2_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e5m2,
    ]

    ctx.enqueue_function_checked[kernel, kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e4m3_e5m2_f16_64x8x32
# CHECK: 10464.0 10944.0 11408.0 11848.0 12256.0 12624.0 13232.0 13800.0
# CHECK: 10944.0 11488.0 11968.0 12432.0 12872.0 13280.0 13904.0 14512.0
# CHECK: 11408.0 11968.0 12504.0 12992.0 13448.0 13888.0 14560.0 15184.0
# CHECK: 11848.0 12432.0 12992.0 13528.0 14016.0 14464.0 15160.0 15824.0
# CHECK: 12384.0 13000.0 13576.0 14144.0 14672.0 15152.0 15888.0 16576.0
# CHECK: 12880.0 13536.0 14144.0 14720.0 15280.0 15808.0 16576.0 17312.0
# CHECK: 13328.0 14032.0 14680.0 15288.0 15856.0 16416.0 17216.0 17984.0
# CHECK: 13904.0 14608.0 15304.0 15952.0 16544.0 17120.0 17984.0 18784.0
# CHECK: 14448.0 15184.0 15888.0 16576.0 17216.0 17808.0 18672.0 19536.0
# CHECK: 14928.0 15728.0 16448.0 17152.0 17824.0 18464.0 19360.0 20224.0
# CHECK: 15384.0 16208.0 17008.0 17712.0 18400.0 19072.0 20016.0 20912.0
# CHECK: 15824.0 16656.0 17472.0 18256.0 18976.0 19648.0 20624.0 21552.0
# CHECK: 16360.0 17232.0 18064.0 18864.0 19632.0 20336.0 21344.0 22320.0
# CHECK: 16864.0 17760.0 18624.0 19440.0 20240.0 20992.0 22032.0 23040.0
# CHECK: 17312.0 18272.0 19168.0 20016.0 20816.0 21600.0 22688.0 23712.0
# CHECK: 17888.0 18848.0 19792.0 20672.0 21504.0 22288.0 23440.0 24528.0
# CHECK: 18432.0 19424.0 20368.0 21296.0 22160.0 22976.0 24128.0 25280.0
# CHECK: 18912.0 19968.0 20944.0 21872.0 22784.0 23632.0 24816.0 25952.0
# CHECK: 19360.0 20448.0 21504.0 22448.0 23360.0 24256.0 25472.0 26640.0
# CHECK: 19808.0 20896.0 21968.0 22992.0 23920.0 24832.0 26080.0 27280.0
# CHECK: 20336.0 21472.0 22544.0 23584.0 24608.0 25504.0 26800.0 28048.0
# CHECK: 20832.0 22000.0 23104.0 24160.0 25184.0 26176.0 27488.0 28768.0
# CHECK: 21280.0 22496.0 23648.0 24736.0 25760.0 26768.0 28160.0 29456.0
# CHECK: 21856.0 23072.0 24272.0 25392.0 26464.0 27472.0 28896.0 30272.0
# CHECK: 22416.0 23648.0 24848.0 26016.0 27104.0 28160.0 29600.0 31008.0
# CHECK: 22880.0 24208.0 25424.0 26592.0 27728.0 28800.0 30272.0 31680.0
# CHECK: 23328.0 24672.0 25984.0 27168.0 28320.0 29424.0 30928.0 32368.0
# CHECK: 23776.0 25120.0 26448.0 27712.0 28880.0 30000.0 31536.0 33024.0
# CHECK: 24304.0 25696.0 27024.0 28304.0 29552.0 30688.0 32256.0 33760.0
# CHECK: 24816.0 26224.0 27600.0 28880.0 30144.0 31360.0 32960.0 34496.0
# CHECK: 25264.0 26736.0 28128.0 29456.0 30720.0 31952.0 33632.0 35168.0
# CHECK: 25840.0 27312.0 28752.0 30112.0 31424.0 32640.0 34368.0 36000.0
# CHECK: 26400.0 27888.0 29328.0 30736.0 32064.0 33344.0 35040.0 36736.0
# CHECK: 26864.0 28448.0 29904.0 31312.0 32688.0 33984.0 35744.0 37408.0
# CHECK: 27312.0 28912.0 30464.0 31888.0 33280.0 34592.0 36384.0 38112.0
# CHECK: 27760.0 29360.0 30928.0 32448.0 33824.0 35200.0 36992.0 38752.0
# CHECK: 28160.0 29808.0 31376.0 32896.0 34368.0 35744.0 37568.0 39360.0
# CHECK: 28528.0 30208.0 31808.0 33344.0 34848.0 36288.0 38112.0 39904.0
# CHECK: 29120.0 30832.0 32464.0 34048.0 35552.0 36992.0 38976.0 40768.0
# CHECK: 29712.0 31424.0 33088.0 34688.0 36224.0 37696.0 39680.0 41632.0
# CHECK: 30240.0 32016.0 33696.0 35328.0 36864.0 38368.0 40384.0 42304.0
# CHECK: 30736.0 32544.0 34272.0 35904.0 37504.0 39008.0 41056.0 43008.0
# CHECK: 31200.0 33024.0 34816.0 36512.0 38080.0 39648.0 41696.0 43712.0
# CHECK: 31632.0 33504.0 35296.0 37056.0 38688.0 40256.0 42336.0 44352.0
# CHECK: 32032.0 33920.0 35776.0 37504.0 39200.0 40800.0 42912.0 44928.0
# CHECK: 32656.0 34592.0 36448.0 38240.0 39936.0 41600.0 43776.0 45824.0
# CHECK: 33280.0 35200.0 37088.0 38912.0 40672.0 42336.0 44576.0 46720.0
# CHECK: 33824.0 35840.0 37728.0 39552.0 41344.0 43040.0 45280.0 47488.0
# CHECK: 34368.0 36384.0 38336.0 40192.0 41984.0 43712.0 45984.0 48192.0
# CHECK: 34848.0 36928.0 38912.0 40800.0 42592.0 44352.0 46656.0 48896.0
# CHECK: 35328.0 37408.0 39424.0 41376.0 43200.0 44960.0 47296.0 49568.0
# CHECK: 35712.0 37888.0 39936.0 41888.0 43776.0 45568.0 47936.0 50208.0
# CHECK: 36128.0 38272.0 40384.0 42400.0 44288.0 46144.0 48512.0 50816.0
# CHECK: 36512.0 38688.0 40800.0 42848.0 44800.0 46656.0 49088.0 51424.0
# CHECK: 37120.0 39328.0 41440.0 43520.0 45504.0 47392.0 49920.0 52288.0
# CHECK: 37696.0 39936.0 42080.0 44160.0 46144.0 48096.0 50624.0 53088.0
# CHECK: 38208.0 40512.0 42688.0 44768.0 46784.0 48736.0 51328.0 53824.0
# CHECK: 38720.0 41024.0 43264.0 45408.0 47424.0 49376.0 51968.0 54528.0
# CHECK: 39200.0 41536.0 43776.0 45984.0 48064.0 50016.0 52640.0 55168.0
# CHECK: 39616.0 42016.0 44288.0 46496.0 48640.0 50656.0 53248.0 55808.0
# CHECK: 40000.0 42432.0 44768.0 47008.0 49120.0 51200.0 53856.0 56416.0
# CHECK: 40640.0 43072.0 45440.0 47712.0 49888.0 51968.0 54752.0 57344.0
# CHECK: 41248.0 43712.0 46080.0 48384.0 50624.0 52736.0 55488.0 58240.0
# CHECK: 41824.0 44320.0 46720.0 49024.0 51264.0 53440.0 56256.0 58944.0
def wgmma_e4m3_e5m2_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e4m3_e5m2_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e4m3fn,
        b_type = DType.float8_e5m2,
    ]

    ctx.enqueue_function_checked[kernel, kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


# CHECK-LABEL: wgmma_e5m2_e4m3_f16_64x8x32
# CHECK: 10464.0 10944.0 11408.0 11848.0 12384.0 12880.0 13328.0 13904.0
# CHECK: 10944.0 11488.0 11968.0 12432.0 13000.0 13536.0 14032.0 14608.0
# CHECK: 11408.0 11968.0 12504.0 12992.0 13576.0 14144.0 14680.0 15304.0
# CHECK: 11848.0 12432.0 12992.0 13528.0 14144.0 14720.0 15288.0 15952.0
# CHECK: 12256.0 12872.0 13448.0 14016.0 14672.0 15280.0 15856.0 16544.0
# CHECK: 12624.0 13280.0 13888.0 14464.0 15152.0 15808.0 16416.0 17120.0
# CHECK: 13232.0 13904.0 14560.0 15160.0 15888.0 16576.0 17216.0 17984.0
# CHECK: 13800.0 14512.0 15184.0 15824.0 16576.0 17312.0 17984.0 18784.0
# CHECK: 14328.0 15080.0 15784.0 16448.0 17248.0 18000.0 18720.0 19536.0
# CHECK: 14824.0 15608.0 16352.0 17056.0 17856.0 18656.0 19392.0 20256.0
# CHECK: 15296.0 16104.0 16880.0 17616.0 18464.0 19264.0 20048.0 20944.0
# CHECK: 15736.0 16576.0 17376.0 18144.0 19024.0 19872.0 20656.0 21600.0
# CHECK: 16144.0 17024.0 17840.0 18624.0 19552.0 20416.0 21248.0 22192.0
# CHECK: 16768.0 17680.0 18544.0 19360.0 20320.0 21216.0 22080.0 23088.0
# CHECK: 17376.0 18304.0 19200.0 20048.0 21056.0 22000.0 22896.0 23936.0
# CHECK: 17920.0 18912.0 19840.0 20704.0 21744.0 22720.0 23664.0 24736.0
# CHECK: 18448.0 19456.0 20432.0 21344.0 22384.0 23408.0 24368.0 25488.0
# CHECK: 18944.0 19984.0 20992.0 21936.0 23008.0 24048.0 25056.0 26192.0
# CHECK: 19424.0 20480.0 21504.0 22496.0 23616.0 24672.0 25696.0 26880.0
# CHECK: 19840.0 20960.0 22016.0 23008.0 24160.0 25280.0 26336.0 27520.0
# CHECK: 20240.0 21376.0 22480.0 23504.0 24672.0 25824.0 26912.0 28128.0
# CHECK: 20624.0 21776.0 22896.0 23968.0 25168.0 26320.0 27440.0 28704.0
# CHECK: 21232.0 22416.0 23552.0 24656.0 25920.0 27104.0 28240.0 29568.0
# CHECK: 21792.0 23024.0 24192.0 25296.0 26592.0 27840.0 29008.0 30336.0
# CHECK: 22304.0 23584.0 24800.0 25936.0 27232.0 28512.0 29744.0 31104.0
# CHECK: 22816.0 24096.0 25344.0 26528.0 27872.0 29152.0 30416.0 31840.0
# CHECK: 23296.0 24608.0 25872.0 27088.0 28480.0 29792.0 31056.0 32512.0
# CHECK: 23728.0 25088.0 26384.0 27616.0 29040.0 30400.0 31696.0 33152.0
# CHECK: 24128.0 25520.0 26848.0 28112.0 29552.0 30944.0 32272.0 33760.0
# CHECK: 24768.0 26176.0 27536.0 28848.0 30336.0 31744.0 33088.0 34656.0
# CHECK: 25360.0 26816.0 28192.0 29536.0 31072.0 32528.0 33920.0 35488.0
# CHECK: 25920.0 27408.0 28832.0 30176.0 31744.0 33248.0 34688.0 36288.0
# CHECK: 26432.0 27968.0 29424.0 30816.0 32384.0 33920.0 35392.0 37056.0
# CHECK: 26944.0 28480.0 29984.0 31408.0 33024.0 34560.0 36064.0 37760.0
# CHECK: 27408.0 28992.0 30496.0 31968.0 33600.0 35200.0 36704.0 38432.0
# CHECK: 27840.0 29456.0 31008.0 32480.0 34176.0 35776.0 37344.0 39072.0
# CHECK: 28224.0 29888.0 31472.0 32992.0 34688.0 36352.0 37952.0 39712.0
# CHECK: 28608.0 30272.0 31904.0 33472.0 35200.0 36864.0 38496.0 40320.0
# CHECK: 28944.0 30656.0 32288.0 33856.0 35648.0 37344.0 38976.0 40800.0
# CHECK: 29248.0 30992.0 32656.0 34240.0 36032.0 37792.0 39424.0 41280.0
# CHECK: 29552.0 31296.0 32992.0 34624.0 36416.0 38208.0 39872.0 41760.0
# CHECK: 29824.0 31600.0 33312.0 34976.0 36800.0 38592.0 40288.0 42208.0
# CHECK: 30592.0 32384.0 34112.0 35776.0 37728.0 39520.0 41248.0 43264.0
# CHECK: 31296.0 33152.0 34912.0 36608.0 38560.0 40448.0 42208.0 44224.0
# CHECK: 31936.0 33856.0 35680.0 37376.0 39360.0 41280.0 43136.0 45184.0
# CHECK: 32576.0 34496.0 36352.0 38144.0 40128.0 42048.0 43904.0 46048.0
# CHECK: 33184.0 35136.0 36992.0 38816.0 40864.0 42816.0 44672.0 46816.0
# CHECK: 33760.0 35744.0 37632.0 39488.0 41568.0 43552.0 45440.0 47616.0
# CHECK: 34272.0 36320.0 38272.0 40096.0 42208.0 44256.0 46208.0 48384.0
# CHECK: 34784.0 36832.0 38816.0 40704.0 42848.0 44896.0 46880.0 49120.0
# CHECK: 35264.0 37344.0 39360.0 41280.0 43456.0 45536.0 47552.0 49792.0
# CHECK: 35712.0 37824.0 39840.0 41792.0 44032.0 46144.0 48160.0 50432.0
# CHECK: 36096.0 38272.0 40320.0 42304.0 44544.0 46720.0 48768.0 51072.0
# CHECK: 36480.0 38656.0 40768.0 42816.0 45056.0 47232.0 49344.0 51712.0
# CHECK: 36832.0 39040.0 41152.0 43232.0 45504.0 47712.0 49824.0 52224.0
# CHECK: 37184.0 39392.0 41536.0 43616.0 45952.0 48160.0 50304.0 52704.0
# CHECK: 37440.0 39744.0 41920.0 43968.0 46336.0 48576.0 50752.0 53184.0
# CHECK: 38240.0 40512.0 42752.0 44864.0 47264.0 49536.0 51776.0 54272.0
# CHECK: 38976.0 41312.0 43552.0 45696.0 48160.0 50496.0 52736.0 55264.0
# CHECK: 39680.0 42048.0 44352.0 46496.0 48992.0 51392.0 53696.0 56224.0
# CHECK: 40320.0 42752.0 45056.0 47296.0 49792.0 52224.0 54560.0 57184.0
# CHECK: 40960.0 43392.0 45760.0 48000.0 50560.0 52992.0 55360.0 58016.0
# CHECK: 41536.0 44032.0 46400.0 48704.0 51264.0 53760.0 56128.0 58816.0
# CHECK: 42112.0 44608.0 47040.0 49344.0 51968.0 54464.0 56896.0 59584.0
def wgmma_e5m2_e4m3_f16_64x8x32(ctx: DeviceContext):
    print("== wgmma_e5m2_e4m3_f16_64x8x32")
    alias M = 64
    alias N = 8
    alias K = 32

    var lhs = ManagedLayoutTensor[
        DType.float8_e5m2,
        Layout.row_major(M, K),
    ](ctx)

    var lhs_tensor = lhs.tensor()

    @parameter
    for i in range(M):

        @parameter
        for j in range(K):
            lhs_tensor[i, j] = i + j

    var rhs = ManagedLayoutTensor[
        DType.float8_e4m3fn,
        Layout.row_major(K, N),
    ](ctx)

    var rhs_tensor = rhs.tensor()

    @parameter
    for i in range(K):

        @parameter
        for j in range(N):
            rhs_tensor[i, j] = i + j

    var res = ManagedLayoutTensor[
        DType.float16,
        Layout.row_major(M, N),
    ](ctx)

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-A.png
    alias a_smem_layout = Layout(
        IntTuple(IntTuple(8, 8), IntTuple(16, 2)),
        IntTuple(IntTuple(16, 128), IntTuple(1, 1024)),
    )

    # https://docs.nvidia.com/cuda/parallel-thread-execution/_images/wgmma-64N32-core-matrices-B.png
    alias b_smem_layout = Layout(
        IntTuple(IntTuple(16, 2), 8), IntTuple(IntTuple(1, 128), 16)
    )

    alias kernel = wgmma_f16_kernel[
        M,
        N,
        K,
        64,
        8,
        32,
        a_smem_layout,
        b_smem_layout,
        a_type = DType.float8_e5m2,
        b_type = DType.float8_e4m3fn,
    ]

    ctx.enqueue_function_checked[kernel, kernel](
        lhs.device_tensor(),
        rhs.device_tensor(),
        res.device_tensor(),
        grid_dim=(1, 1),
        block_dim=(128),
    )
    ctx.synchronize()

    print(res.tensor())
    _ = rhs^
    _ = lhs^
    _ = res^


def main():
    with DeviceContext() as ctx:
        wgmma_e4m3_e4m3_f16_64x8x32(ctx)
        wgmma_e5m2_e5m2_f16_64x8x32(ctx)
        wgmma_e4m3_e5m2_f16_64x8x32(ctx)
        wgmma_e5m2_e4m3_f16_64x8x32(ctx)
