load("//bazel:api.bzl", "modular_py_test", "modular_run_binary_test", "requirement")

modular_py_test(
    name = "tests-pure",
    size = "large",
    srcs = [
        "conftest.py",
        "test_cli.py",
        "test_config_pure.py",
        "test_context.py",
        "test_cumsum.py",
        "test_dtype.py",
        "test_engine.py",
        "test_explicit_device.py",
        "test_graph_telemetry.py",
        "test_huggingface_utilities_pure.py",
        "test_load_weights.py",
        "test_memory_estimation.py",
        "test_registry.py",
        "test_sampling.py",
        "test_telemetry.py",
        "test_tokenizer.py",
    ],
    data = [
        "//PyTorch:TorchRuntimePlugins",
        "//SDK/integration-test/API:inputs",
        "//SDK/integration-test/API:pytest_inputs",
        "//SDK/lib/API/python/tests/graph/testdata",
    ],
    env = {
        "MODULAR_PATH": ".",
        "GRAPH_TESTDATA": "SDK/lib/API/python/tests/graph/testdata",
    },
    env_inherit = ["HF_TOKEN"],
    gpu_constraints = [
        "//:has_gpu",
    ],
    tags = [
        "gpu",
        "no-sandbox",
        "requires-network",
    ],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max",
        "//SDK/lib/API/python/max/entrypoints",
        # TODO(MAXPLAT-85): mypy doesn't inherit requirements
        requirement("click"),
        requirement("gguf"),
        requirement("hypothesis"),
        requirement("safetensors"),
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [requirement("xgrammar")],  # needed only for test_sampling.py
        "//conditions:default": [],
    }),
)

# TODO(AITLIB-349): Fix this test. For some reason it doesn't work on AMD, so
# we created a separate Bazel target and configured it to run only on NVIDIA GPUs.
modular_py_test(
    name = "test-speculative-decoding",
    size = "large",
    srcs = [
        "conftest.py",
        "test_speculative_decoding.py",
    ],
    data = [
        # TODO(MAXPLAT-86): Find a shared place for these tools
        "//PyTorch:TorchRuntimePlugins",
        "//SDK/integration-test/API:inputs",
        "//SDK/integration-test/API:pytest_inputs",
    ],
    env = {"MODULAR_PATH": "."},
    env_inherit = ["HF_TOKEN"],
    gpu_constraints = [
        "//:has_gpu",
        "//:nvidia_gpu",
    ],
    tags = [
        "gpu",
        "no-sandbox",
        "requires-network",
    ],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max",
        "//SDK/lib/API/python/max/entrypoints",
        # TODO(MAXPLAT-85): mypy doesn't inherit requirements
        requirement("click"),
        requirement("gguf"),
        requirement("safetensors"),
        requirement("torch"),
        requirement("transformers"),
    ],
)

modular_py_test(
    name = "tests-hf",
    size = "large",
    srcs = [
        "conftest.py",
        "test_config.py",
        "test_huggingface_utilities.py",
    ],
    data = [
        # TODO(MAXPLAT-86): Find a shared place for these tools
        "//PyTorch:TorchRuntimePlugins",
        "//SDK/integration-test/API:inputs",
        "//SDK/integration-test/API:pytest_inputs",
    ],
    env = {"MODULAR_PATH": "."},
    env_inherit = ["HF_TOKEN"],
    gpu_constraints = ["//:has_gpu"],
    tags = [
        "AITLIB-293",
        "gpu",
        "manual",  # TODO(AITLIB-293): Remove this once we have a way to run these tests on CI.
        "no-sandbox",
        "requires-network",
    ],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max",
        "//SDK/lib/API/python/max/entrypoints",
        # TODO(MAXPLAT-85): mypy doesn't inherit requirements
        requirement("click"),
        requirement("gguf"),
        requirement("safetensors"),
        requirement("torch"),
        requirement("transformers"),
    ],
)

modular_py_test(
    name = "multi-gpu-tests",
    size = "large",
    srcs = glob(["multi_gpu_tests/**/*.py"]),
    data = [
        "//ModularFramework/tools/max",
        "//PyTorch:TorchRuntimePlugins",
        "//SDK/integration-test:test_user_op",
        "//SDK/integration-test/API:inputs",
        "//SDK/integration-test/API:pytest_inputs",
        "//SDK/lib/API/python/tests/graph/testdata",
    ],
    env = {
        "CUSTOM_OPS_PATH": "$(rootpath //SDK/integration-test:test_user_op)",
        "MODULAR_PATH": ".",
        "GRAPH_TESTDATA": "SDK/lib/API/python/tests/graph/testdata",
    },
    gpu_constraints = ["//:has_4_gpus"],
    tags = ["gpu"],
    deps = [
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/nn",
        requirement("numpy"),
    ],
)

modular_run_binary_test(
    name = "pipelines_multi_gpu_smoke_test",
    args = [
        "generate",
        "--model-path",
        "hf-internal-testing/tiny-random-LlamaForCausalLM",
        "--devices=gpu:0,1,2,3",
        "--max-batch-size=1",
        "--max-new-tokens=32",
        "--max-num-steps=1",
        "--max-length=512",
    ],
    binary = "//SDK/lib/API/python/max/entrypoints:pipelines",
    gpu_constraints = ["//:has_4_gpus"],
    tags = [
        "AITLIB-293",
        "gpu",
        "manual",
        "no-sandbox",
        "requires-network",
    ],
)

modular_run_binary_test(
    name = "pipelines_multi_gpu_smoke_test_with_subgraphs",
    args = [
        "generate",
        "--model-path",
        "hf-internal-testing/tiny-random-LlamaForCausalLM",
        "--devices=gpu:0,1,2,3",
        "--max-batch-size=1",
        "--max-new-tokens=32",
        "--max-num-steps=1",
        "--max-length=512",
        "--use-subgraphs",
    ],
    binary = "//SDK/lib/API/python/max/entrypoints:pipelines",
    gpu_constraints = ["//:has_4_gpus"],
    tags = [
        "AITLIB-293",
        "gpu",
        "manual",
        "no-sandbox",
        "requires-network",
    ],
)

modular_run_binary_test(
    name = "pipelines_speculative_decoding_smoke_test",
    args = [
        "generate",
        "--draft-model-path=hf-internal-testing/tiny-random-LlamaForCausalLM",
        "--model-path=hf-internal-testing/tiny-random-LlamaForCausalLM",
        "--quantization-encoding=bfloat16",
        "--devices=gpu",
        "--cache-strategy=paged",
        "--kv-cache-page-size=128",
        "--max-batch-size=4",
        "--max-num-steps=10",
        "--max-length=512",
        "--device-memory-utilization=0.3",
        "--prompt=\"The meaning of life is\"",
    ],
    binary = "//SDK/lib/API/python/max/entrypoints:pipelines",
    gpu_constraints = ["//:has_gpu"],
    tags = [
        "gpu",
        "no-sandbox",
        "requires-network",
    ],
)
