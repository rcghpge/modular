# Llama3 tests.

load(
    "//bazel:api.bzl",
    "modular_py_test",
    "requirement",
)

package(default_visibility = [
    "//:__pkg__",
    "//SDK/integration-test:__subpackages__",
    "//max/tests:__subpackages__",
    "//oss/modular/max/tests:__subpackages__",
])

modular_py_test(
    name = "tests_gpu",
    size = "enormous",
    srcs = glob(
        ["test_*_gpu.py"],
    ) + [
        "conftest.py",
    ],
    data = [
        "//max/tests/integration/architectures/llama3/testdata",
    ],
    env = {
        "MODULAR_PATH": ".",
        "PIPELINES_TESTDATA": (
            "max/tests/integration/architectures/llama3/testdata"
        ),
    },
    exec_properties = {
        "test.resources:gpu-memory": "20",
    },
    gpu_constraints = ["//:has_gpu"] + select({
        "//:apple_gpu": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),  # FIXME: MOCO-2411
    ignore_extra_deps = [
        # Used as a pytest plugin
        "//max/tests/integration/test_common:registry",
    ],
    tags = [
        "gpu",
        "no-pydeps",  # vllm is conditionally included via select()
        "no-sandbox",
        "requires-network",
    ],
    deps = [
        "//max/python/max/driver",
        "//max/python/max/dtype",
        "//max/python/max/engine",
        "//max/python/max/graph",
        "//max/python/max/interfaces",
        "//max/python/max/kv_cache",
        "//max/python/max/nn",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/test_common:context_utils",
        "//max/tests/integration/test_common:graph_utils",
        "//max/tests/integration/test_common:registry",
        requirement("hypothesis"),
        requirement("torch"),
        requirement("numpy"),
        requirement("transformers"),
    ] + select({
        "//:nvidia_gpu": [
            requirement("vllm"),
        ],
        "//conditions:default": [],
    }),
)
