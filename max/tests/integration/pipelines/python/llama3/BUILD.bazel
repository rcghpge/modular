# Llama3 tests.

load(
    "//bazel:api.bzl",
    "modular_py_binary",
    "modular_py_test",
    "requirement",
)

modular_py_binary(
    name = "evaluate_llama",
    srcs = ["evaluate_llama.py"],
    data = [
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//SDK/integration-test/pipelines/python/llama3/testdata",
        "//Support/tools/system-info",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    env = {
        "MODULAR_MOJO_MAX_ORCRT_PATH": (
            "$(location @llvm-project//compiler-rt:orc_rt)"
        ),
        "MODULAR_MOJO_MAX_COMPILERRT_PATH": "$(location //KGEN:CompilerRT)",
        "MODULAR_CUDA_QUERY_PATH": "$(location //Kernels/tools/gpu-query)",
        "MODULAR_SYSTEM_INFO_PATH": "$(location //Support/tools/system-info)",
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama3/testdata"
        ),
    },
    imports = ["."],
    visibility = ["//visibility:public"],
    deps = [
        "//SDK/public/max-repo/pipelines/python/llama3",
        requirement("click"),
        requirement("py-cpuinfo"),
    ],
)

modular_py_binary(
    name = "verify",
    srcs = ["verify.py"],
    data = [
        "@test_llama_golden",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama3/testdata"
        ),
    },
    imports = ["."],
    visibility = ["//visibility:public"],
    deps = [
        ":evaluate_llama",
        "//GenericML/tools/model",
        requirement("click"),
    ],
)

[
    modular_py_test(
        name = src.split("/")[-1].split(".")[0],
        size = "enormous",
        srcs = [
            "conftest.py",
            "torch_utils.py",
            "torch_vision_encoder.py",
            src,
        ],
        data = [
            "//KGEN:CompilerRT",
            "//ModularFramework/tools/mof",
            "//SDK/integration-test/pipelines/python/llama3/testdata",
            "//Support/tools/system-info",
            "@llvm-project//compiler-rt:orc_rt",
            "@test_llama_golden",
        ],
        env = {
            "BAZEL": "true",
            "MODULAR_PATH": ".",
            "PIPELINES_TESTDATA": (
                "SDK/integration-test/pipelines/python/llama3/testdata"
            ),
            "MODULAR_SYSTEM_INFO_PATH": (
                "$(location //Support/tools/system-info)"
            ),
        },
        exec_compatible_with = ["//:large"],
        requires_gpu = False,
        tags = [
            "requires-network",
        ],
        target_compatible_with = select(
            {
                # TODO: Sanitizer libs aren't available when python loads pybind shared libs
                "//:asan": ["@platforms//:incompatible"],
                "//:tsan": ["@platforms//:incompatible"],
                "//conditions:default": [],
            },
        ),
        deps = [
            ":evaluate_llama",
            "//SDK/integration-test/API/python:modular_graph_test",
            "//SDK/lib/API/python/max/engine",
            "//SDK/lib/API/python/max/serve",
            "//SDK/public/max-repo/pipelines/python/dataprocessing",
            "//SDK/public/max-repo/pipelines/python/llama3",
            "//SDK/public/max-repo/pipelines/python/nn",
            "//SDK/public/max-repo/pipelines/python/utils",
            "//Support/python:support",
            requirement("hypothesis"),
            requirement("transformers"),
            requirement("pytest-asyncio"),
            requirement("huggingface_hub"),
        ],
    )
    for src in glob(
        ["**/test_*.py"],
        exclude = [
            "**/*_gpu.py",
        ],
    )
]

modular_py_test(
    name = "tests_gpu",
    size = "enormous",
    srcs = glob(
        ["test_*_gpu.py"],
    ) + [
        "conftest.py",
        "torch_utils.py",
        "torch_vision_encoder.py",
    ],
    data = [
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//ModularFramework/tools/mof",
        "//SDK/integration-test/pipelines/python/llama3/testdata",
        "//Support/tools/system-info",
        "@llvm-project//compiler-rt:orc_rt",
        "@test_llama_golden",
        "@torch_llama_golden",
    ],
    env = {
        "BAZEL": "true",
        "MODULAR_CUDA_QUERY_PATH": "$(location //Kernels/tools/gpu-query)",
        "MODULAR_SYSTEM_INFO_PATH": "$(location //Support/tools/system-info)",
        "MODULAR_PATH": ".",
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama3/testdata"
        ),
    },
    requires_gpu = True,
    tags = [
        "A100",
        "gpu",
        "requires-network",
    ],
    target_compatible_with = select(
        {
            # TODO: Sanitizer libs aren't available when python loads pybind shared libs
            "//:asan": ["@platforms//:incompatible"],
            "//:tsan": ["@platforms//:incompatible"],
            "//conditions:default": [],
        },
    ),
    deps = [
        ":evaluate_llama",
        "//SDK/lib/API/python/max/engine",
        "//SDK/public/max-repo/pipelines/python/llama3",
        "//SDK/public/max-repo/pipelines/python/utils",
        "//Support/python:support",
        requirement("hypothesis"),
        requirement("transformers"),
        requirement("py-cpuinfo"),
        requirement("pytest-asyncio"),
    ],
)
