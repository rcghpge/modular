# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""Verifies the logit files generated by evaluting llama.

Example run:

```
MODEL=llama3_1
ENCODING=float32

./bazelw run SDK/integration-test/pipelines/python/llama3:evaluate_llama -- --model $MODEL --encoding $ENCODING
./bazelw run SDK/integration-test/pipelines/python/llama3/testdata:run_torch_llama  -- --model $MODEL --encoding $ENCODING
./bazelw run SDK/integration-test/pipelines/python/llama3:verify -- \
    /tmp/${MODEL}_${ENCODING}_all_golden.json \
    /tmp/torch_${MODEL}_${ENCODING}_all_golden.json \
    --eval-metric=tol,cos,kl
```
"""

from pathlib import Path
from typing import List, Optional, Sequence, TypedDict

import click
import numpy as np
from model.cli.commands._internal._verify_utils import construct_validator
from model.utils.custom_args import CommaSeparatedList
from model.utils.exceptions import AccuracyError
from model.utils.logging import CONSOLE
from test_common.distance_metrics import kl_divergence_from_logits
from test_common.evaluate import compare_values
from test_common.numpy_encoder import NumpyDecoder
from typing_extensions import NotRequired


class TokenInfo(TypedDict):
    """Information about a token in the output."""

    next_token: int
    """The next token in the output."""
    next_token_logits: float
    """The logits for the next token."""
    logits: np.ndarray
    """The logits for the token."""


class ModelOutput(TypedDict):
    """The prompt and the output of a model run."""

    prompt: str
    """The prompt that was used to generate the output."""
    values: NotRequired[List[TokenInfo]]
    """Outputs from a text generation model."""
    embeddings: NotRequired[np.ndarray]
    """Outputs from a text embedding model."""


@click.command()
@click.argument("pipeline_outputs", type=Path)
@click.argument("torch_outputs", type=Path)
@click.option(
    "--eval-metric",
    type=CommaSeparatedList,
    default="tol",
    show_default=True,
    help=(
        "The metric(s) that should be used to verify model output correctness."
        " Options include Relative/Absolute tolerance (tol), Cosine"
        " Similarity (cos), or KL Divergence (kl). You may specify more than"
        " one with commas seperating each entry."
    ),
)
@click.option(
    "--relative-tolerance",
    type=float,
    default=1e-03,
    help="The relative tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--absolute-tolerance",
    type=float,
    default=1e-04,
    help="The absolute tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--cos-dist-threshold",
    type=float,
    default=1e-3,
    help=(
        "The threshold for cosine similarity across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--kl-div-threshold",
    type=float,
    default=1e-3,
    help=(
        "The threshold for cosine similarity across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--diff-count",
    type=int,
    default=4,
    help="Print the first N entries where the results do not match.",
    show_default=True,
)
@click.option(
    "--print-suggested-tolerances",
    is_flag=True,
    default=False,
    help=(
        "On failure, prints a set of potential tolerances based on the pareto"
        " frontier of passing absolute and relative tolerance combinations."
    ),
)
def main(
    pipeline_outputs: Path,
    torch_outputs: Path,
    eval_metric: CommaSeparatedList,
    relative_tolerance: float,
    absolute_tolerance: float,
    cos_dist_threshold: float,
    kl_div_threshold: float,
    diff_count: int,
    print_suggested_tolerances: bool,
):
    first_framework = "Pipeline"
    other_framework = "Torch"
    validator = construct_validator(
        eval_metric,
        atol=absolute_tolerance,
        rtol=relative_tolerance,
        cos_threshold=cos_dist_threshold,
        kl_div_threshold=kl_div_threshold,
    )

    pipeline_results: List[ModelOutput] = NumpyDecoder().decode(
        pipeline_outputs.read_text()
    )
    torch_results: List[ModelOutput] = NumpyDecoder().decode(
        torch_outputs.read_text()
    )

    results = []
    any_failed = False

    def compare(pipeline_value, torch_value, description):
        nonlocal any_failed
        if isinstance(pipeline_value, (int, float)):
            if pipeline_value != torch_value:
                pf = "âš ï¸" if description.startswith("'next_token'") else ""
                print(
                    f"{pf}Got mismatching {description}: {pipeline_value} !="
                    f" {torch_value}"
                )
            return

        validation_result = validator(pipeline_value, torch_value)
        results.append(validation_result)
        if validation_result.any_failed():
            CONSOLE.print(
                f"===Got an error for the computed {description}."
                f"\n{validation_result.get_failure_messages()}"
            )
            any_failed = True

    compare_values(pipeline_results, torch_results, compare_fn=compare)

    if any_failed:
        validator.print_error_table(
            first_framework,
            other_framework,
            results,
            diff_count=diff_count,
            print_suggested_tolerances=print_suggested_tolerances,
        )

        if diff_count == 0:
            CONSOLE.print(
                "Use the --diff_count=N option to get more details on"
                " where the results differ."
            )

        print_discrepancy_report(pipeline_results, torch_results)
        raise AccuracyError(
            f'The outputs when using the "{first_framework}"'
            " framework is not within tolerance to the"
            f' outputs when using the "{other_framework}"'
            " framework."
        )

    print_discrepancy_report(pipeline_results, torch_results)
    # the results are within tolerance, so we log a good message
    CONSOLE.print(
        f'ðŸ‘ The outputs when using the "{first_framework}" framework'
        f' are close to the outputs when using the "{other_framework}"'
        " framework for specified tolerances"
        f" ({validator.threshold_str()})"
    )


def print_discrepancy_report(
    results: List[ModelOutput], references: List[ModelOutput]
) -> None:
    """Calculate and print discrepancy metrics between outputs from two model runs.

    Args:
        results: List of model outputs to evaluate
        references: List of reference model outputs to compare against
    """
    if len(results) != len(references):
        raise ValueError("The two lists must have the same length")

    mae_per_prompt, rmse_per_prompt, kl_div_per_prompt = [], [], []
    for result, reference in zip(results, references):
        verify_matching_prompts(result, reference)
        kl_div = None
        if "embeddings" in result and "embeddings" in reference:
            mae, rmse = calculate_mae_and_rmse(
                result["embeddings"].astype(np.float64),
                reference["embeddings"].astype(np.float64),
            )
        elif "values" in result and "values" in reference:
            mae, rmse, kl_div = calculate_logit_discrepancies(
                result["values"], reference["values"]
            )
        else:
            raise ValueError(
                "Unknown model output type, did not find embeddings or values"
            )
        mae_per_prompt.append(mae)
        rmse_per_prompt.append(rmse)
        if kl_div is not None:
            kl_div_per_prompt.append(kl_div)

    kl_div_parameter = (
        None if len(kl_div_per_prompt) == 0 else kl_div_per_prompt
    )
    print_discrepancy_results(mae_per_prompt, rmse_per_prompt, kl_div_parameter)


def verify_matching_prompts(
    result: ModelOutput, reference: ModelOutput
) -> None:
    """Verify that the prompts match between result and reference.

    Args:
        result: Model output from the model being evaluated
        reference: Model output from the reference model

    Returns:
        None
    """
    if result["prompt"] != reference["prompt"]:
        raise ValueError(
            f"Mismatched prompts:\nResult: {result['prompt']}\nReference: {reference['prompt']}"
        )


def calculate_mae_and_rmse(
    result: np.ndarray, reference: np.ndarray
) -> tuple[float, float]:
    """Calculate MAE and RMSE between result and reference embeddings.

    Args:
        result: Vector of floats
        reference: Reference vector of floats

    Returns:
        A tuple containing (MAE, RMSE) as float values
    """
    diff = result - reference
    mae = np.mean(np.abs(diff))
    rmse = np.sqrt(np.mean(diff**2))
    return float(mae), float(rmse)


def calculate_logit_discrepancies(
    result_values: List[TokenInfo], reference_values: List[TokenInfo]
) -> tuple[float, float, float]:
    """Calculate MAE, RMSE and KL Divergence between result and reference logits.

    Args:
        result_values: List of token logits from the model being evaluated
        reference_values: List of token logits from the reference model

    Returns:
        A tuple containing (MAE, RMSE, KL_divergence) as float values
    """
    total_mae = 0.0
    total_rmse = 0.0
    total_kl_div = 0.0
    steps = 0

    for res_token, ref_token in zip(result_values, reference_values):
        res_logits_float64 = res_token["logits"].astype(np.float64)
        ref_logits_float64 = ref_token["logits"].astype(np.float64)
        mae, rmse = calculate_mae_and_rmse(
            res_logits_float64, ref_logits_float64
        )

        total_mae += mae
        total_rmse += rmse
        total_kl_div += kl_divergence_from_logits(
            res_logits_float64, ref_logits_float64
        )
        steps += 1

        # If the tokens diverge, stop computing the discrepancies
        if res_token["next_token"] != ref_token["next_token"]:
            break

    mae_average = total_mae / steps
    rmse_average = total_rmse / steps
    kl_div_average = total_kl_div / steps

    return float(mae_average), float(rmse_average), float(kl_div_average)


def print_discrepancy_results(
    mae_per_prompt: Sequence[float],
    rmse_per_prompt: Sequence[float],
    kl_div_per_prompt: Optional[Sequence[float]],
) -> None:
    """Print discrepancy results in a standardized format.

    Args:
        mae_per_prompt: Sequence of mean absolute error values
        rmse_per_prompt: Sequence of root mean square error values
        kl_div_per_prompt: Optional sequence of KL divergence values

    Returns:
        None
    """
    # TODO: Add reference values as to what are good/ok/bad numbers once
    # we gain experience with them

    # Determine if we're working with logits or embeddings
    output_type = "logit" if kl_div_per_prompt is not None else "embedding"

    # Calculate averages
    # TODO: These averages can be misleading if the number of steps is different
    # for each prompt (happens when there's a token mismatch).
    avg_mae = sum(mae_per_prompt) / len(mae_per_prompt)
    avg_rmse = sum(rmse_per_prompt) / len(rmse_per_prompt)

    # Format the per-prompt values
    formatter = "{:.2e}".format
    formatted_mae = ", ".join(formatter(error) for error in mae_per_prompt)
    formatted_rmse = ", ".join(formatter(error) for error in rmse_per_prompt)

    CONSOLE.print(f"\n===== {output_type} discrepancy report =====\n")

    if kl_div_per_prompt is not None:
        avg_kl_div = sum(kl_div_per_prompt) / len(kl_div_per_prompt)
        formatted_kl_div = ", ".join(
            formatter(error) for error in kl_div_per_prompt
        )
        CONSOLE.print(
            f"KL Div: {formatter(avg_kl_div)} <â€” Use this number if unsure"
        )

    CONSOLE.print(f"RMSE:   {formatter(avg_rmse)}")
    CONSOLE.print(f"MAE:    {formatter(avg_mae)}")
    CONSOLE.print("\nPer prompt discrepancy numbers:")

    if kl_div_per_prompt is not None:
        CONSOLE.print(f"KL Div: [{formatted_kl_div}]")

    CONSOLE.print(f"RMSE:   [{formatted_rmse}]")
    CONSOLE.print(f"MAE:    [{formatted_mae}]")

    CONSOLE.print("\nLower numbers are better.")
    CONSOLE.print(
        f"They measure how close the {output_type}s are between the two frameworks."
    )

    if kl_div_per_prompt is not None:
        CONSOLE.print(
            "The numbers are computed until the first token mismatch, if any."
        )

    CONSOLE.print("\n===== end discrepancy report =====\n")


if __name__ == "__main__":
    main()
