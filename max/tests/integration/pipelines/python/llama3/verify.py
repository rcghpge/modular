# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""Verifies the logit files generated by evaluting llama.

Example run:

```
MODEL=llama3_1
ENCODING=float32

./bazelw run SDK/integration-test/pipelines/python/llama3:evaluate_llama -- --model $MODEL --encoding $ENCODING
./bazelw run SDK/integration-test/pipelines/python/llama3/testdata:run_torch_llama  -- --model $MODEL --encoding $ENCODING
./bazelw run SDK/integration-test/pipelines/python/llama3:verify -- \
    /tmp/${MODEL}_${ENCODING}_all_golden.json \
    /tmp/torch_${MODEL}_${ENCODING}_all_golden.json \
    --eval-metric=tol,cos,kl
```
"""

from dataclasses import dataclass
from pathlib import Path
from typing import List, Literal, Optional, Sequence, TypedDict, TypeVar

import click
import numpy as np
from model.cli.commands._internal._verify_utils import construct_validator
from model.utils.custom_args import CommaSeparatedList
from model.utils.exceptions import AccuracyError
from model.utils.logging import CONSOLE
from test_common.distance_metrics import kl_divergence_from_logits
from test_common.evaluate import compare_values
from test_common.numpy_encoder import NumpyDecoder
from typing_extensions import NotRequired


class TokenInfo(TypedDict):
    """Information about a token in the output."""

    next_token: int
    """The next token in the output."""
    next_token_logits: float
    """The logits for the next token."""
    logits: np.ndarray
    """The logits for the token."""


class ModelOutput(TypedDict):
    """The prompt and the output of a model run."""

    prompt: str
    """The prompt that was used to generate the output."""
    values: NotRequired[List[TokenInfo]]
    """Outputs from a text generation model."""
    embeddings: NotRequired[np.ndarray]
    """Outputs from a text embedding model."""


# Shared defaults between CLI and verify function
DEFAULT_EVAL_METRIC = ["tol"]
DEFAULT_RELATIVE_TOLERANCE = 1e-03
DEFAULT_ABSOLUTE_TOLERANCE = 1e-04
DEFAULT_COS_DIST_THRESHOLD = 1e-3
DEFAULT_KL_DIV_THRESHOLD = 1e-3
DEFAULT_DIFF_COUNT = 4
DEFAULT_PRINT_SUGGESTED_TOLERANCES = False


@click.command()
@click.argument("pipeline_outputs", type=Path)
@click.argument("torch_outputs", type=Path)
@click.option(
    "--eval-metric",
    type=CommaSeparatedList,
    default=DEFAULT_EVAL_METRIC,
    show_default=True,
    help=(
        "The metric(s) that should be used to verify model output correctness."
        " Options include Relative/Absolute tolerance (tol), Cosine"
        " Similarity (cos), or KL Divergence (kl). You may specify more than"
        " one with commas seperating each entry."
    ),
)
@click.option(
    "--relative-tolerance",
    type=float,
    default=DEFAULT_RELATIVE_TOLERANCE,
    help="The relative tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--absolute-tolerance",
    type=float,
    default=DEFAULT_ABSOLUTE_TOLERANCE,
    help="The absolute tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--cos-dist-threshold",
    type=float,
    default=DEFAULT_COS_DIST_THRESHOLD,
    help=(
        "The threshold for cosine similarity across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--kl-div-threshold",
    type=float,
    default=DEFAULT_KL_DIV_THRESHOLD,
    help=(
        "The threshold for KL divergence across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--diff-count",
    type=int,
    default=DEFAULT_DIFF_COUNT,
    help="Print the first N entries where the results do not match.",
    show_default=True,
)
@click.option(
    "--print-suggested-tolerances",
    is_flag=True,
    default=DEFAULT_PRINT_SUGGESTED_TOLERANCES,
    help=(
        "On failure, prints a set of potential tolerances based on the pareto"
        " frontier of passing absolute and relative tolerance combinations."
    ),
)
def main(
    pipeline_outputs: Path,
    torch_outputs: Path,
    eval_metric: CommaSeparatedList,
    relative_tolerance: float,
    absolute_tolerance: float,
    cos_dist_threshold: float,
    kl_div_threshold: float,
    diff_count: int,
    print_suggested_tolerances: bool,
) -> None:
    """Click command entry point that delegates to the implementation function.

    This wrapper exists because Click command functions aren't easily picklable,
    which causes issues when called from multiprocessing.
    """

    result = verify(
        pipeline_outputs=pipeline_outputs,
        torch_outputs=torch_outputs,
        eval_metric=eval_metric,
        relative_tolerance=relative_tolerance,
        absolute_tolerance=absolute_tolerance,
        cos_dist_threshold=cos_dist_threshold,
        kl_div_threshold=kl_div_threshold,
        diff_count=diff_count,
        print_suggested_tolerances=print_suggested_tolerances,
    )

    if not result.passed:
        raise AccuracyError(result.error_message)


@dataclass(frozen=True)
class DiscrepancyReport:
    """Contains discrepancy metrics between model outputs."""

    model_modality: Literal["logit", "embedding"]
    """Type of model output modality being compared ('logit' or 'embedding')."""

    mae_per_prompt: List[float]
    """Mean absolute error for each prompt."""

    rmse_per_prompt: List[float]
    """Root mean square error for each prompt."""

    kl_div_per_prompt: Optional[List[float]] = None
    """KL divergence for each prompt (only for logit outputs)."""

    @property
    def avg_mae(self) -> float:
        """Calculate average mean absolute error across all prompts."""
        return sum(self.mae_per_prompt) / len(self.mae_per_prompt)

    @property
    def avg_rmse(self) -> float:
        """Calculate average root mean square error across all prompts."""
        return sum(self.rmse_per_prompt) / len(self.rmse_per_prompt)

    @property
    def avg_kl_div(self) -> Optional[float]:
        """Calculate average KL divergence across all prompts (only for logit outputs)."""
        if self.kl_div_per_prompt is None:
            return None
        return sum(self.kl_div_per_prompt) / len(self.kl_div_per_prompt)


@dataclass(frozen=True)
class VerificationResult:
    """Result of model output verification."""

    passed: bool
    """Whether the verification passed or failed."""

    discrepancy_report: DiscrepancyReport
    """Report containing discrepancy metrics between model outputs."""

    error_message: Optional[str] = None
    """Error message if verification failed, None otherwise."""


def verify(
    pipeline_outputs: Path,
    torch_outputs: Path,
    eval_metric: Optional[Sequence[str]] = None,
    relative_tolerance: Optional[float] = None,
    absolute_tolerance: Optional[float] = None,
    cos_dist_threshold: Optional[float] = None,
    kl_div_threshold: Optional[float] = None,
    diff_count: Optional[int] = None,
    print_suggested_tolerances: Optional[bool] = None,
) -> VerificationResult:
    """Verify that pipeline outputs match torch outputs within specified tolerances.

    Args:
        pipeline_outputs: Path to the pipeline outputs JSON file
        torch_outputs: Path to the torch outputs JSON file
        eval_metric: Metrics to use for evaluation (e.g., ["tol", "cos", "kl"])
        relative_tolerance: Relative tolerance for numerical comparison
        absolute_tolerance: Absolute tolerance for numerical comparison
        cos_dist_threshold: Threshold for cosine similarity
        kl_div_threshold: Threshold for KL divergence
        diff_count: Number of differences to show in error output
        print_suggested_tolerances: Whether to print suggested tolerances on failure

    Returns:
        VerificationResult containing pass/fail status and discrepancy report
    """

    # MyPy needs the TypeVar in order to infer the type of the return value
    T = TypeVar("T")

    def val_or(value: Optional[T], default: T) -> T:
        return value if value is not None else default

    # Note: These default value shenanigans are here to simplify the logic
    # of the caller in generate_llm_logits.py.
    eval_metric = val_or(eval_metric, DEFAULT_EVAL_METRIC)
    relative_tolerance = val_or(relative_tolerance, DEFAULT_RELATIVE_TOLERANCE)
    absolute_tolerance = val_or(absolute_tolerance, DEFAULT_ABSOLUTE_TOLERANCE)
    cos_dist_threshold = val_or(cos_dist_threshold, DEFAULT_COS_DIST_THRESHOLD)
    kl_div_threshold = val_or(kl_div_threshold, DEFAULT_KL_DIV_THRESHOLD)
    diff_count = val_or(diff_count, DEFAULT_DIFF_COUNT)
    print_suggested_tolerances = val_or(
        print_suggested_tolerances, DEFAULT_PRINT_SUGGESTED_TOLERANCES
    )

    first_framework = "Pipeline"
    other_framework = "Torch"
    validator = construct_validator(
        eval_metric,
        atol=absolute_tolerance,
        rtol=relative_tolerance,
        cos_threshold=cos_dist_threshold,
        kl_div_threshold=kl_div_threshold,
    )

    pipeline_results: List[ModelOutput] = NumpyDecoder().decode(
        pipeline_outputs.read_text()
    )
    torch_results: List[ModelOutput] = NumpyDecoder().decode(
        torch_outputs.read_text()
    )

    results = []
    any_failed = False

    def compare(pipeline_value, torch_value, description):
        nonlocal any_failed
        if isinstance(pipeline_value, (int, float)):
            if pipeline_value != torch_value:
                pf = "âš ï¸" if description.startswith("'next_token'") else ""
                print(
                    f"{pf}Got mismatching {description}: {pipeline_value} !="
                    f" {torch_value}"
                )
            return

        validation_result = validator(pipeline_value, torch_value)
        results.append(validation_result)
        if validation_result.any_failed():
            CONSOLE.print(
                f"===Got an error for the computed {description}."
                f"\n{validation_result.get_failure_messages()}"
            )
            any_failed = True

    compare_values(pipeline_results, torch_results, compare_fn=compare)

    report = compute_discrepancy_report(pipeline_results, torch_results)
    print_discrepancy_report(report)

    error_message = None
    test_passed = True
    if any_failed:
        test_passed = False
        validator.print_error_table(
            first_framework,
            other_framework,
            results,
            diff_count=diff_count,
            print_suggested_tolerances=print_suggested_tolerances,
        )

        if diff_count == 0:
            CONSOLE.print(
                "Use the --diff_count=N option to get more details on"
                " where the results differ."
            )

        error_message = (
            f'The outputs when using the "{first_framework}"'
            " framework is not within tolerance to the"
            f' outputs when using the "{other_framework}"'
            " framework."
        )
        CONSOLE.print(error_message)

    # the results are within tolerance, so we log a good message
    CONSOLE.print(
        f'ðŸ‘ The outputs when using the "{first_framework}" framework'
        f' are close to the outputs when using the "{other_framework}"'
        " framework for specified tolerances"
        f" ({validator.threshold_str()})"
    )

    return VerificationResult(
        passed=test_passed,
        discrepancy_report=report,
        error_message=error_message,
    )


def compute_discrepancy_report(
    results: List[ModelOutput], references: List[ModelOutput]
) -> DiscrepancyReport:
    """Calculate discrepancy metrics between outputs from two model runs.

    Args:
        results: List of model outputs to evaluate
        references: List of reference model outputs to compare against

    Returns:
        A DiscrepancyReport containing the calculated metrics
    """
    if len(results) != len(references):
        raise ValueError("The two lists must have the same length")

    mae_per_prompt, rmse_per_prompt, kl_div_per_prompt = [], [], []
    model_modality: Optional[Literal["logit", "embedding"]] = None
    for result, reference in zip(results, references):
        verify_matching_prompts(result, reference)
        kl_div = None
        if "embeddings" in result and "embeddings" in reference:
            mae, rmse = calculate_mae_and_rmse(
                result["embeddings"].astype(np.float64),
                reference["embeddings"].astype(np.float64),
            )
            model_modality = "embedding"
        elif "values" in result and "values" in reference:
            mae, rmse, kl_div = calculate_logit_discrepancies(
                result["values"], reference["values"]
            )
            model_modality = "logit"
        else:
            raise ValueError(
                "Unknown model modality, did not find embeddings or values"
            )
        mae_per_prompt.append(mae)
        rmse_per_prompt.append(rmse)
        if kl_div is not None:
            kl_div_per_prompt.append(kl_div)

    if model_modality is None:
        raise ValueError("Could not determine model modality")

    return DiscrepancyReport(
        mae_per_prompt=mae_per_prompt,
        rmse_per_prompt=rmse_per_prompt,
        kl_div_per_prompt=kl_div_per_prompt if kl_div_per_prompt else None,
        model_modality=model_modality,
    )


def verify_matching_prompts(
    result: ModelOutput, reference: ModelOutput
) -> None:
    """Verify that the prompts match between result and reference.

    Args:
        result: Model output from the model being evaluated
        reference: Model output from the reference model

    Returns:
        None
    """
    if result["prompt"] != reference["prompt"]:
        raise ValueError(
            f"Mismatched prompts:\nResult: {result['prompt']}\nReference: {reference['prompt']}"
        )


def calculate_mae_and_rmse(
    result: np.ndarray, reference: np.ndarray
) -> tuple[float, float]:
    """Calculate MAE and RMSE between result and reference embeddings.

    Args:
        result: Vector of floats
        reference: Reference vector of floats

    Returns:
        A tuple containing (MAE, RMSE) as float values
    """
    diff = result - reference
    mae = np.mean(np.abs(diff))
    rmse = np.sqrt(np.mean(diff**2))
    return float(mae), float(rmse)


def calculate_logit_discrepancies(
    result_values: List[TokenInfo], reference_values: List[TokenInfo]
) -> tuple[float, float, float]:
    """Calculate MAE, RMSE and KL Divergence between result and reference logits.

    Args:
        result_values: List of token logits from the model being evaluated
        reference_values: List of token logits from the reference model

    Returns:
        A tuple containing (MAE, RMSE, KL_divergence) as float values
    """
    total_mae = 0.0
    total_rmse = 0.0
    total_kl_div = 0.0
    steps = 0

    for res_token, ref_token in zip(result_values, reference_values):
        res_logits_float64 = res_token["logits"].astype(np.float64)
        ref_logits_float64 = ref_token["logits"].astype(np.float64)
        mae, rmse = calculate_mae_and_rmse(
            res_logits_float64, ref_logits_float64
        )

        total_mae += mae
        total_rmse += rmse
        total_kl_div += kl_divergence_from_logits(
            res_logits_float64, ref_logits_float64
        )
        steps += 1

        # If the tokens diverge, stop computing the discrepancies
        if res_token["next_token"] != ref_token["next_token"]:
            break

    mae_average = total_mae / steps
    rmse_average = total_rmse / steps
    kl_div_average = total_kl_div / steps

    return float(mae_average), float(rmse_average), float(kl_div_average)


def print_discrepancy_report(report: DiscrepancyReport) -> None:
    """Print discrepancy metrics in a standardized format.

    Args:
        report: DiscrepancyReport containing the metrics to print
    """
    # Format the per-prompt values
    formatter = "{:.2e}".format
    formatted_mae = ", ".join(
        formatter(error) for error in report.mae_per_prompt
    )
    formatted_rmse = ", ".join(
        formatter(error) for error in report.rmse_per_prompt
    )

    CONSOLE.print(f"\n===== {report.model_modality} discrepancy report =====\n")

    if report.kl_div_per_prompt is not None:
        formatted_kl_div = ", ".join(
            formatter(error) for error in report.kl_div_per_prompt
        )
        CONSOLE.print(
            f"KL Div: {formatter(report.avg_kl_div)} <â€” Use this number if unsure"
        )

    CONSOLE.print(f"RMSE:   {formatter(report.avg_rmse)}")
    CONSOLE.print(f"MAE:    {formatter(report.avg_mae)}")
    CONSOLE.print("\nPer prompt discrepancy numbers:")

    if report.kl_div_per_prompt is not None:
        CONSOLE.print(f"KL Div: [{formatted_kl_div}]")

    CONSOLE.print(f"RMSE:   [{formatted_rmse}]")
    CONSOLE.print(f"MAE:    [{formatted_mae}]")

    CONSOLE.print("\nLower numbers are better.")
    CONSOLE.print(
        f"They measure how close the {report.model_modality}s are between the two frameworks."
    )

    if report.kl_div_per_prompt is not None:
        CONSOLE.print(
            "The numbers are computed until the first token mismatch, if any."
        )

    CONSOLE.print("\n===== end discrepancy report =====\n")


if __name__ == "__main__":
    main()
