# ===----------------------------------------------------------------------=== #
#
# This file is Modular Inc proprietary.
#
# ===----------------------------------------------------------------------=== #
"""Verifies the logit files generated by evaluting llama.

Example run:

```
MODEL=llama3_1
ENCODING=float32

./bazelw run SDK/integration-test/pipelines/python/llama3:evaluate_llama -- --model $MODEL --encoding $ENCODING
./bazelw run SDK/integration-test/pipelines/python/llama3/testdata:run_torch_llama  -- --model $MODEL --encoding $ENCODING
./bazelw run SDK/integration-test/pipelines/python/llama3:verify -- \
    /tmp/${MODEL}_${ENCODING}_all_golden.json \
    /tmp/torch_${MODEL}_${ENCODING}_all_golden.json \
    --eval-metric=tol,cos,kl
```
"""

from pathlib import Path

import click
from test_common.numpy_encoder import NumpyDecoder
from test_common.evaluate import compare_values
from model.cli.commands._internal._verify_utils import construct_validator
from model.utils.custom_args import CommaSeparatedList
from model.utils.exceptions import AccuracyError
from model.utils.logging import CONSOLE


@click.command()
@click.argument("pipeline_outputs", type=Path)
@click.argument("torch_outputs", type=Path)
@click.option(
    "--eval-metric",
    type=CommaSeparatedList,
    default="tol",
    show_default=True,
    help=(
        "The metric(s) that should be used to verify model output correctness."
        " Options include Relative/Absolute tolerance (tol), Cosine"
        " Similarity (cos), or KL Divergence (kl). You may specify more than"
        " one with commas seperating each entry."
    ),
)
@click.option(
    "--relative-tolerance",
    type=float,
    default=1e-03,
    help="The relative tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--absolute-tolerance",
    type=float,
    default=1e-04,
    help="The absolute tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--cos-dist-threshold",
    type=float,
    default=1e-3,
    help=(
        "The threshold for cosine similarity across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--kl-div-threshold",
    type=float,
    default=1e-3,
    help=(
        "The threshold for cosine similarity across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--diff-count",
    type=int,
    default=4,
    help="Print the first N entries where the results do not match.",
    show_default=True,
)
@click.option(
    "--print-suggested-tolerances",
    is_flag=True,
    default=False,
    help=(
        "On failure, prints a set of potential tolerances based on the pareto"
        " frontier of passing absolute and relative tolerance combinations."
    ),
)
def main(
    pipeline_outputs: Path,
    torch_outputs: Path,
    eval_metric: CommaSeparatedList,
    relative_tolerance: float,
    absolute_tolerance: float,
    cos_dist_threshold: float,
    kl_div_threshold: float,
    diff_count: int,
    print_suggested_tolerances: bool,
):
    first_framework = "Pipeline"
    other_framework = "Torch"
    validator = construct_validator(
        eval_metric,
        atol=absolute_tolerance,
        rtol=relative_tolerance,
        cos_threshold=cos_dist_threshold,
        kl_div_threshold=kl_div_threshold,
    )

    pipeline_results = NumpyDecoder().decode(pipeline_outputs.read_text())
    torch_results = NumpyDecoder().decode(torch_outputs.read_text())

    results = []
    any_failed = False

    def compare(pipeline_value, torch_value, description):
        nonlocal any_failed
        if isinstance(pipeline_value, (int, float)):
            if pipeline_value != torch_value:
                print(
                    f"Got mismatching {description}: {pipeline_value} !="
                    f" {torch_value}"
                )
            return

        validation_result = validator(pipeline_value, torch_value)
        results.append(validation_result)
        if validation_result.any_failed():
            CONSOLE.print(
                f"===Got an error for the computed {description}."
                f"\n{validation_result.get_failure_messages()}"
            )
            any_failed = True

    compare_values(pipeline_results, torch_results, compare_fn=compare)

    if any_failed:
        validator.print_error_table(
            first_framework,
            other_framework,
            results,
            diff_count=diff_count,
            print_suggested_tolerances=print_suggested_tolerances,
        )

        if diff_count == 0:
            CONSOLE.print(
                "Use the --diff_count=N option to get more details on"
                " where the results differ."
            )

        raise AccuracyError(
            f'The outputs when using the "{first_framework}"'
            " framework is not within tolerance to the"
            f' outputs when using the "{other_framework}"'
            " framework."
        )

    # the results are within tolerance, so we log a good message
    CONSOLE.print(
        f'üëç The outputs when using the "{first_framework}" framework'
        f' are close to the outputs when using the "{other_framework}"'
        " framework for specified tolerances"
        f" ({validator.threshold_str()})"
    )


if __name__ == "__main__":
    main()
