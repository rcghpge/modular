load("//bazel:api.bzl", "modular_py_binary", "requirement")

modular_py_binary(
    name = "evaluate_batch_scenarios",
    srcs = ["evaluate_batch_scenarios.py"],
    data = [
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//Support/tools/system-info",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    env = {
        "MODULAR_MOJO_MAX_ORCRT_PATH": "$(location @llvm-project//compiler-rt:orc_rt)",
        "MODULAR_MOJO_MAX_COMPILERRT_PATH": "$(location //KGEN:CompilerRT)",
        "MODULAR_CUDA_QUERY_PATH": "$(location //Kernels/tools/gpu-query)",
        "MODULAR_SYSTEM_INFO_PATH": "$(location //Support/tools/system-info)",
    },
    imports = ["."],
    visibility = ["//visibility:public"],
    deps = [
        "//SDK/lib/API/python/max/pipelines",
        "//SDK/lib/API/python/max/serve",
        "//SDK/pipelines/pixtral",
        "//SDK/public/max-repo/pipelines/python/llama3",
        "//SDK/public/max-repo/pipelines/python/utils",
        requirement("click"),
    ],
)

modular_py_binary(
    name = "verify_pipelines",
    srcs = ["verify_pipelines.py"],
    data = [
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//SDK/integration-test/pipelines/python/llama3:evaluate_llama",
        "//SDK/integration-test/pipelines/python/llama3:verify",
        "//SDK/integration-test/pipelines/python/llama3/testdata:run_torch_llama",
        "@llvm-project//compiler-rt:orc_rt",
        "@rules_python//python/runfiles",
        "@torch_llama_golden",
    ],
    env = {
        "LLAMA3_MODULAR_BIN": "$(execpath //SDK/integration-test/pipelines/python/llama3:evaluate_llama)",
        "LLAMA3_TORCH_BIN": "$(execpath //SDK/integration-test/pipelines/python/llama3/testdata:run_torch_llama)",
        "LLAMA3_VERIFY_BIN": "$(execpath //SDK/integration-test/pipelines/python/llama3:verify)",
        # MODULAR_CUDA_QUERY_PATH is required by the Llama3 scripts.
        "MODULAR_CUDA_QUERY_PATH": "$(execpath //Kernels/tools/gpu-query)",
        # PIPELINES_TESTDATA is required by the Llama3 scripts.
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama3/testdata"
        ),
    },
    visibility = ["//visibility:public"],
    deps = [
        requirement("click"),
    ],
)
