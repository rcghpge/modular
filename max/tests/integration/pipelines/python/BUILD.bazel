load("//bazel:api.bzl", "modular_py_binary", "modular_py_test", "requirement")

modular_py_binary(
    name = "evaluate_batch_scenarios",
    srcs = ["evaluate_batch_scenarios.py"],
    data = [
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//Support/tools/system-info",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    env = {
        "MODULAR_MOJO_MAX_ORCRT_PATH": "$(location @llvm-project//compiler-rt:orc_rt)",
        "MODULAR_MOJO_MAX_COMPILERRT_PATH": "$(location //KGEN:CompilerRT)",
        "MODULAR_CUDA_QUERY_PATH": "$(location //Kernels/tools/gpu-query)",
        "MODULAR_SYSTEM_INFO_PATH": "$(location //Support/tools/system-info)",
    },
    imports = ["."],
    visibility = ["//visibility:public"],
    deps = [
        "//SDK/lib/API/python/max/pipelines",
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/pipelines/cli",
        "//SDK/lib/API/python/max/pipelines/llama3",
        "//SDK/lib/API/python/max/pipelines/pixtral",
        "//SDK/lib/API/python/max/serve",
        requirement("click"),
        requirement("huggingface-hub"),
    ],
)

modular_py_binary(
    name = "verify_pipelines",
    srcs = ["verify_pipelines.py"],
    data = [
        ":generate_llm_logits",
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//SDK/integration-test/pipelines/python/llama3:verify",
        "@llvm-project//compiler-rt:orc_rt",
        "@torch_llama3-vision_golden",
        "@torch_llama_golden",
        "@torch_mistral_golden",
        "@torch_mpnet_golden",
        "@torch_pixtral_golden",
        "@torch_replit_golden",
    ],
    env = {
        "GENERATE_LLM_LOGITS_BIN": "$(execpath :generate_llm_logits)",
        "LLAMA3_VERIFY_BIN": "$(execpath //SDK/integration-test/pipelines/python/llama3:verify)",
        # MODULAR_CUDA_QUERY_PATH is required by the Llama3 scripts.
        "MODULAR_CUDA_QUERY_PATH": "$(execpath //Kernels/tools/gpu-query)",
        # PIPELINES_TESTDATA is required by the Llama3 scripts.
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama3/testdata"
        ),
    },
    visibility = ["//visibility:public"],
    deps = [
        requirement("click"),
        "@rules_python//python/runfiles",
    ],
)

modular_py_test(
    name = "test_pipelines_cli",
    size = "enormous",
    srcs = ["test_pipelines_cli.py"],
    data = [
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    tags = [
        "huggingface",
        "no-sandbox",
        "requires-network",
    ],
    target_compatible_with = select({
        "//:asan": ["@platforms//:incompatible"],
        "//:tsan": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        "//SDK/lib/API/python/max/pipelines:max_pipelines",
        requirement("transformers"),
        requirement("gguf"),
    ],
)

modular_py_test(
    name = "test_pipelines_cli_lightweight",
    size = "medium",
    srcs = ["test_pipelines_cli_lightweight.py"],
    target_compatible_with = select({
        "//:asan": ["@platforms//:incompatible"],
        "//:tsan": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        "//SDK/lib/API/python/max/pipelines:max_pipelines",
        requirement("transformers"),
        requirement("gguf"),
    ],
)

modular_py_binary(
    name = "generate_llm_logits",
    srcs = [
        "generate_llm_logits.py",
        "replit_compat.py",
    ],
    data = [
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "//Kernels/tools/gpu-query",
        "//SDK/integration-test/pipelines/python/llama3/testdata",
        "//Support/tools/system-info",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    env = {
        "MODULAR_CUDA_QUERY_PATH": "$(location //Kernels/tools/gpu-query)",
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama3/testdata"
        ),
    },
    imports = ["."],
    main = "generate_llm_logits.py",
    visibility = ["//visibility:public"],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/pipelines/llama3",
        "//SDK/lib/API/python/max/pipelines/mistral",
        requirement("click"),
        requirement("einops"),  # Req'd by replit
        requirement("pillow"),
        requirement("requests"),
        requirement("transformers"),
        requirement("torch"),
    ],
)

modular_py_test(
    name = "test_generate_llm_logits_gpu",
    size = "medium",
    srcs = ["test_generate_llm_logits_gpu.py"],
    data = [
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    gpu_constraints = ["//:has_gpu"],
    tags = [
        "gpu",
        "huggingface",
        "no-remote-exec",
        "no-sandbox",
        "requires-network",
    ],
    target_compatible_with = select({
        "//:asan": ["@platforms//:incompatible"],
        "//:tsan": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        ":generate_llm_logits",
        requirement("click"),
    ],
)

modular_py_binary(
    name = "lm-eval",
    srcs = ["run_lm_eval.py"],
    ignore_collisions = True,  # duplicate examples/__init__.py from deps
    main = "run_lm_eval.py",
    # Some of lm-eval's optional deps are not picked up by Bazel for some
    # reason so we have to include them explicitly here.
    deps = [
        requirement(r)
        for r in [
            "lm-eval",
            # lm-eval[api] dependencies
            "requests",
            "aiohttp",
            "tenacity",
            "tqdm",
            "tiktoken",
            # lm-eval[ifeval] dependencies
            "langdetect",
            "immutabledict",
            "nltk",
            # lm-eval[math] dependencies
            "sympy",
            "antlr4-python3-runtime",
        ]
    ],
)

modular_py_binary(
    name = "pipelines-lm-eval",
    srcs = ["pipelines_lm_eval.py"],
    data = [
        "run_lm_eval.py",
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "//SDK/integration-test/pipelines/python/eval_tasks",
        "//SDK/lib/API/python/max/pipelines:max_pipelines",
        "@llvm-project//compiler-rt:orc_rt",
        "@mistral-evals//:evaluate",
    ],
    ignore_collisions = True,  # duplicate examples/__init__.py from deps
    main = "pipelines_lm_eval.py",
    deps = [
        ":lm-eval",
        "//SDK/integration-test/pipelines/python/eval_tasks/human_eval",
        "@mistral-evals//:evaluate",
        "@rules_python//python/runfiles",
        requirement("click"),
        requirement("requests"),
    ],
)

modular_py_test(
    name = "test_pipelines_lm_eval",
    size = "medium",
    srcs = ["test_pipelines_lm_eval.py"],
    data = [
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    ignore_collisions = True,  # duplicate examples/__init__.py from deps
    tags = [
        "huggingface",
        "no-sandbox",
        "requires-network",
    ],
    target_compatible_with = select({
        "//:asan": ["@platforms//:incompatible"],
        "//:tsan": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        ":pipelines-lm-eval",
        requirement("click"),
    ],
)

modular_py_binary(
    name = "pipelines_mteb",
    srcs = ["pipelines_mteb.py"],
    data = [
        "//GenericML:MGPRT",
        "//KGEN:CompilerRT",
        "//SDK/lib/API/python/max/pipelines:max_pipelines",
        "@llvm-project//compiler-rt:orc_rt",
    ],
    ignore_collisions = True,  # duplicate scripts/__init__.py from deps
    deps = [
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/pipelines/cli",
        "//SDK/lib/API/python/max/pipelines/dataprocessing",
        requirement("click"),
        requirement("mteb"),
    ],
)
