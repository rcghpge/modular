# Llama4 tests.

load(
    "//bazel:api.bzl",
    "modular_py_test",
    "requirement",
)

modular_py_test(
    name = "llama4",
    size = "large",
    srcs = glob(["**/test_*.py"]) + ["conftest.py"],
    data = [
        "//ModularFramework/tools/max",
        "//SDK/integration-test/pipelines/python/llama4/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "SDK/integration-test/pipelines/python/llama4/testdata"
        ),
    },
    gpu_constraints = ["//:has_gpu"],
    tags = [
        "gpu",
        "no-sandbox",
    ],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/entrypoints",
        "//SDK/lib/API/python/max/nn",
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/serve",
        "//Support/python:support",
        requirement("torch"),
        requirement("transformers"),
    ],
)
