load("//bazel:api.bzl", "modular_py_test", "requirement")

modular_py_test(
    name = "attention_tests",
    srcs = glob(
        ["*.py"],
        exclude = [
            "*_gpu.py",
            "test_attention_no_opaque.py",
        ],
    ),
    deps = [
        "//SDK/integration-test/API/python/graph:modular_graph_test",
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/nn",
        "//SDK/lib/API/python/max/pipelines",
        "//SDK/lib/API/python/max/serve",
        requirement("torch"),
        requirement("transformers"),
        requirement("hypothesis"),
        requirement("gguf"),
        requirement("pytest-asyncio"),
    ],
)

modular_py_test(
    name = "attention_gpu_tests",
    timeout = "long",
    srcs = glob(
        ["*_gpu.py"],
        exclude = ["test_attention_fp8_amd_gpu.py"],
    ) + ["conftest.py"],
    env = {
        "MODULAR_TORCH_MEMORY_PERCENT": "0.5",
    },
    exec_properties = {
        "test.resources:gpu-memory": "18",
    },
    gpu_constraints = ["//:has_gpu"],
    tags = ["gpu"],
    deps = [
        "//SDK/integration-test/API/python/graph:modular_graph_test",
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/nn",
        "//SDK/lib/API/python/max/pipelines",
        "//SDK/lib/API/python/max/serve",
        requirement("torch"),
        requirement("transformers"),
        requirement("hypothesis"),
        requirement("gguf"),
        requirement("pytest-asyncio"),
    ],
)

modular_py_test(
    name = "attention_no_opaque_tests",
    timeout = "long",
    srcs = ["test_attention_no_opaque.py"],
    deps = [
        "//SDK/integration-test/API/python/graph:modular_graph_test",
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/nn",
        "//SDK/lib/API/python/max/pipelines",
        "//SDK/lib/API/python/max/serve",
        requirement("torch"),
        requirement("transformers"),
        requirement("hypothesis"),
        requirement("gguf"),
        requirement("pytest-asyncio"),
    ],
)

modular_py_test(
    name = "attention_amd_tests",
    size = "large",
    srcs = [
        "conftest.py",
        "test_attention_fp8_amd_gpu.py",
    ],
    exec_properties = {
        "test.resources:gpu-memory": "2",
    },
    tags = ["gpu"],
    target_compatible_with = ["//:amd_gpu"],
    deps = [
        "//SDK/integration-test/API/python/graph:modular_graph_test",
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/nn",
        "//SDK/lib/API/python/max/pipelines",
        requirement("torch"),
        requirement("numpy"),
    ],
)
