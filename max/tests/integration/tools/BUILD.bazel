load("//bazel:api.bzl", "modular_py_binary", "modular_py_library", "modular_py_test", "requirement")

package(default_visibility = [
    "//:__pkg__",
    "//SDK/integration-test:__subpackages__",
    "//max/tests:__subpackages__",
    "//oss/modular/max/tests:__subpackages__",
])

modular_py_library(
    name = "hf_config_overrides",
    testonly = True,
    srcs = [
        "hf_config_overrides.py",
    ],
    deps = [
        "//max/python/max/nn",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        requirement("transformers"),
    ],
)

modular_py_library(
    name = "debugging_utils",
    testonly = True,
    srcs = [
        "debugging_utils.py",
    ],
    imports = ["."],
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        ":hf_config_overrides",
        ":run_models",
        "//max/python/max/driver",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/architectures/idefics3:torch_utils",
        "//max/tests/integration/architectures/internvl:torch_utils",
        "//max/tests/integration/architectures/qwen2_5vl:generate_utils",
        "//max/tests/integration/architectures/qwen3vl:generate_utils",
        "//max/tests/integration/test_common",
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("diffusers"),  # Required for image generation pipelines
        requirement("einops"),  # Req'd by replit
        requirement("peft"),  # Required for LoRA support
        requirement("pillow"),
        requirement("requests"),
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [
            requirement("datasets"),  # Required by gptqmodel
            requirement("device-smi"),  # Required by gptqmodel
            requirement("gptqmodel"),
            requirement("logbar"),  # Required by gptqmodel
            requirement("optimum"),
            requirement("threadpoolctl"),  # Required by gptqmodel
            requirement("tokenicer"),  # Required by gptqmodel
        ],
        "//conditions:default": [],
    }),
)

modular_py_library(
    name = "create_pipelines",
    testonly = True,
    srcs = ["create_pipelines.py"],
    ignore_extra_deps = [
        requirement("einops"),  # Req'd by replit
        requirement("timm"),  # Required by InternVL.
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
    ],
    imports = ["."],
    deps = [
        "//max/python/max/interfaces",
        "//max/python/max/pipelines",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/core",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/architectures/idefics3:torch_utils",
        "//max/tests/integration/architectures/internvl:torch_utils",
        "//max/tests/integration/architectures/qwen2_5vl:generate_utils",
        "//max/tests/integration/architectures/qwen3vl:generate_utils",
        "//max/tests/integration/test_common:test_data",
        "//max/tests/integration/test_common:torch_utils",
        requirement("diffusers"),  # Required for image generation pipelines
        requirement("einops"),  # Req'd by replit
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("huggingface-hub"),
        requirement("transformers"),
        requirement("peft"),  # Required for LoRA support
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
    ],
)

modular_py_library(
    name = "run_models",
    testonly = True,
    srcs = ["run_models.py"],
    data = [
        "@nvshmem_prebuilt//:host",
    ],
    imports = ["."],
    deps = [
        ":create_pipelines",
        "//max/python/max/interfaces",
        "//max/python/max/pipelines",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration/test_common:evaluate",
        "//max/tests/integration/test_common:vllm_utils",
        requirement("huggingface-hub"),
        requirement("requests"),
        requirement("torch"),
        requirement("typing-extensions"),
    ],
)

modular_py_binary(
    name = "generate_llm_logits",
    testonly = True,
    srcs = [
        "generate_llm_logits.py",
    ],
    data = [
        "//max/tests/integration/architectures/llama3/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "max/tests/integration/architectures/llama3/testdata"
        ),
        # required by models that use Expert Parallelism.
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    imports = ["."],
    main = "generate_llm_logits.py",
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        ":run_models",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/architectures",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration:hf_repo_lock",
        "//max/tests/integration/architectures/idefics3:torch_utils",
        "//max/tests/integration/architectures/internvl:torch_utils",
        "//max/tests/integration/architectures/qwen2_5vl:generate_utils",
        "//max/tests/integration/architectures/qwen3vl:generate_utils",
        "//max/tests/integration/test_common",
        requirement("click"),
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("diffusers"),  # Required for image generation pipelines
        requirement("einops"),  # Req'd by replit
        requirement("peft"),  # Required for LoRA support
        requirement("pillow"),
        requirement("requests"),
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [
            requirement("datasets"),  # Required by gptqmodel
            requirement("device-smi"),  # Required by gptqmodel
            requirement("gptqmodel"),
            requirement("logbar"),  # Required by gptqmodel
            requirement("optimum"),
            requirement("threadpoolctl"),  # Required by gptqmodel
            requirement("tokenicer"),  # Required by gptqmodel
        ],
        "//conditions:default": [],
    }),
)

modular_py_binary(
    name = "debug_model",
    testonly = True,
    srcs = [
        "debug_model.py",
    ],
    data = [
        "//max/tests/integration/architectures/llama3/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "max/tests/integration/architectures/llama3/testdata"
        ),
        # required by models that use Expert Parallelism.
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    imports = ["."],
    main = "debug_model.py",
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":debugging_utils",
        ":hf_config_overrides",
        ":run_models",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/lib",
        requirement("click"),
        requirement("torch"),
    ],
)

modular_py_binary(
    name = "compare_tensors",
    testonly = True,
    srcs = ["compare_tensors.py"],
    imports = ["."],
    main = "compare_tensors.py",
    deps = [
        "//max/python/max/driver",
        requirement("click"),
        requirement("torch"),
    ],
)

modular_py_test(
    name = "test_hf_config_overrides",
    size = "small",
    srcs = ["test_hf_config_overrides.py"],
    imports = ["."],
    tags = [
        "no-pydeps",  # TODO: Fix pydeps import mapping for tools.hf_config_overrides
    ],
    # TODO(MODELS-994): Investigate and enable on macOS
    target_compatible_with = select({
        "@platforms//os:macos": ["@platforms//:incompatible"],
        "//conditions:default": [],
    }),
    deps = [
        "//max/python/max/nn",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration/tools:hf_config_overrides",
        requirement("transformers"),
    ],
)
