load("//bazel:api.bzl", "modular_py_binary", "modular_py_library", "requirement")

package(default_visibility = [
    "//:__pkg__",
    "//SDK/integration-test:__subpackages__",
    "//max/tests:__subpackages__",
    "//oss/modular/max/tests:__subpackages__",
])

modular_py_library(
    name = "hf_config_overrides",
    testonly = True,
    srcs = [
        "hf_config_overrides.py",
    ],
    deps = [
        "//max/python/max/nn",
        "//max/python/max/pipelines/lib",
        "//max/tests/integration/pipelines/python:hf_repo_lock",
        requirement("transformers"),
    ],
)

modular_py_library(
    name = "debugging_utils",
    testonly = True,
    srcs = [
        "debugging_utils.py",
    ],
    imports = ["."],
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        ":hf_config_overrides",
        ":run_models",
        "//max/python/max/driver",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/architectures",
        "//max/tests/integration/pipelines/python:hf_repo_lock",
        "//max/tests/integration/pipelines/python/idefics3:torch_utils",
        "//max/tests/integration/pipelines/python/internvl:torch_utils",
        "//max/tests/integration/pipelines/python/qwen2_5vl:generate_utils",
        "//max/tests/integration/pipelines/python/qwen3vl:generate_utils",
        "//max/tests/integration/pipelines/python/test_common",
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("einops"),  # Req'd by replit
        requirement("peft"),  # Required for LoRA support
        requirement("pillow"),
        requirement("requests"),
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [
            requirement("datasets"),  # Required by gptqmodel
            requirement("device-smi"),  # Required by gptqmodel
            requirement("gptqmodel"),
            requirement("logbar"),  # Required by gptqmodel
            requirement("optimum"),
            requirement("threadpoolctl"),  # Required by gptqmodel
            requirement("tokenicer"),  # Required by gptqmodel
        ],
        "//conditions:default": [],
    }),
)

modular_py_library(
    name = "create_pipelines",
    testonly = True,
    srcs = ["create_pipelines.py"],
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        "//max/python/max/pipelines/architectures",
        "//max/tests/integration/pipelines/python:hf_repo_lock",
        "//max/tests/integration/pipelines/python/idefics3:torch_utils",
        "//max/tests/integration/pipelines/python/internvl:torch_utils",
        "//max/tests/integration/pipelines/python/qwen2_5vl:generate_utils",
        "//max/tests/integration/pipelines/python/qwen3vl:generate_utils",
        "//max/tests/integration/pipelines/python/test_common",
        requirement("einops"),  # Req'd by replit
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
        requirement("peft"),  # Required for LoRA support
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
    ],
)

modular_py_library(
    name = "run_models",
    testonly = True,
    srcs = ["run_models.py"],
    data = [
        "@nvshmem_prebuilt//:host",
    ],
    imports = ["."],
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        "//max/python/max/pipelines/architectures",
        "//max/tests/integration/pipelines/python/test_common",
        requirement("torch"),
        requirement("requests"),
    ],
)

modular_py_binary(
    name = "generate_llm_logits",
    testonly = True,
    srcs = [
        "generate_llm_logits.py",
    ],
    data = [
        "//max/tests/integration/pipelines/python/llama3/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "max/tests/integration/pipelines/python/llama3/testdata"
        ),
        # required by models that use Expert Parallelism.
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    imports = ["."],
    main = "generate_llm_logits.py",
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":create_pipelines",
        ":run_models",
        "//max/python/max/entrypoints",
        "//max/python/max/pipelines/architectures",
        "//max/tests/integration/pipelines/python:hf_repo_lock",
        "//max/tests/integration/pipelines/python/idefics3:torch_utils",
        "//max/tests/integration/pipelines/python/internvl:torch_utils",
        "//max/tests/integration/pipelines/python/qwen2_5vl:generate_utils",
        "//max/tests/integration/pipelines/python/qwen3vl:generate_utils",
        "//max/tests/integration/pipelines/python/test_common",
        requirement("click"),
        requirement("compressed-tensors"),  # Required for compressed-tensors quantization models
        requirement("einops"),  # Req'd by replit
        requirement("peft"),  # Required for LoRA support
        requirement("pillow"),
        requirement("requests"),
        requirement("timm"),  # Required by InternVL.
        requirement("torch"),
        requirement("transformers"),
    ] + select({
        "//:has_gpu": [
            requirement("datasets"),  # Required by gptqmodel
            requirement("device-smi"),  # Required by gptqmodel
            requirement("gptqmodel"),
            requirement("logbar"),  # Required by gptqmodel
            requirement("optimum"),
            requirement("threadpoolctl"),  # Required by gptqmodel
            requirement("tokenicer"),  # Required by gptqmodel
        ],
        "//conditions:default": [],
    }),
)

modular_py_binary(
    name = "debug_model",
    testonly = True,
    srcs = [
        "debug_model.py",
    ],
    data = [
        "//max/tests/integration/pipelines/python/llama3/testdata",
    ],
    env = {
        "PIPELINES_TESTDATA": (
            "max/tests/integration/pipelines/python/llama3/testdata"
        ),
        # required by models that use Expert Parallelism.
        "MODULAR_SHMEM_LIB_DIR": "../+http_archive+nvshmem_prebuilt",
    },
    imports = ["."],
    main = "debug_model.py",
    tags = ["no-pydeps"],  # TODO: Fix and re-enable
    deps = [
        ":debugging_utils",
        ":hf_config_overrides",
        ":run_models",
        "//max/python/max/entrypoints",
        requirement("click"),
        requirement("torch"),
    ],
)
