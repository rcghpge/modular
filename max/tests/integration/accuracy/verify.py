# ===----------------------------------------------------------------------=== #
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
# ===----------------------------------------------------------------------=== #
"""Verifies the logit files generated by evaluating a given model.

To get the logit files, use `generate_llm_logits` once with `torch` and once
with `max`:

```
./bazelw run \
  //max/tests/integration/tools:generate_llm_logits -- \
  --device cpu \
  --framework max \
  --pipeline llama \
  --version llama3_1 \
  --output /tmp/max-goldens.json
```

Remember to change both the framework and the output path when generating the
logit files.

Then, run `verify` with the logit files:
```
./bazelw run \
  //max/tests/integration/accuracy:verify -- \
  --eval-metric cos,kl,tol \
  --relative-tolerance 0 \
  --absolute-tolerance 0 \
  --cos-dist-threshold 0 \
  --kl-div-threshold 0 \
  /tmp/max-goldens.json /tmp/torch-goldens.json
```
"""

import math
from collections.abc import Sequence
from dataclasses import dataclass
from enum import Enum
from pathlib import Path
from typing import Any, TypeVar

import click
import numpy as np
from test_common.custom_args import CommaSeparatedList
from test_common.distance_metrics import kl_divergence_from_logits
from test_common.model_output import (
    ModelOutput,
    ModelOutputView,
    TokenInfo,
    compare_values,
)
from test_common.numpy_encoder import NumpyDecoder
from test_common.table import CONSOLE
from test_common.verify_utils import construct_validator


class VerificationError(click.ClickException):
    def __init__(self, message: str) -> None:
        super().__init__(message)


class AccuracyError(VerificationError):
    def __init__(self, message: str) -> None:
        super().__init__(message)
        self.exit_code = 2


class ModelModality(str, Enum):
    LOGIT = "logit"
    EMBEDDING = "embedding"


# Shared defaults between CLI and verify function
DEFAULT_EVAL_METRIC = ["tol"]
DEFAULT_RELATIVE_TOLERANCE = 1e-03
DEFAULT_ABSOLUTE_TOLERANCE = 1e-04
DEFAULT_COS_DIST_THRESHOLD = 1e-3
DEFAULT_KL_DIV_THRESHOLD = 1e-3
DEFAULT_DIFF_COUNT = 4
DEFAULT_PRINT_SUGGESTED_TOLERANCES = False


@click.command()
@click.argument("pipeline_outputs", type=Path)
@click.argument("torch_outputs", type=Path)
@click.option(
    "--eval-metric",
    type=CommaSeparatedList,
    default=DEFAULT_EVAL_METRIC,
    show_default=True,
    help=(
        "The metric(s) that should be used to verify model output correctness."
        " Options include Relative/Absolute tolerance (tol), Cosine"
        " Similarity (cos), or KL Divergence (kl). You may specify more than"
        " one with commas separating each entry."
    ),
)
@click.option(
    "--relative-tolerance",
    type=float,
    default=DEFAULT_RELATIVE_TOLERANCE,
    help="The relative tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--absolute-tolerance",
    type=float,
    default=DEFAULT_ABSOLUTE_TOLERANCE,
    help="The absolute tolerance used for result verification.",
    show_default=True,
)
@click.option(
    "--cos-dist-threshold",
    type=float,
    default=DEFAULT_COS_DIST_THRESHOLD,
    help=(
        "The threshold for cosine similarity across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--kl-div-threshold",
    type=float,
    default=DEFAULT_KL_DIV_THRESHOLD,
    help=(
        "The threshold for KL divergence across the last axis of the model"
        " output."
    ),
    show_default=True,
)
@click.option(
    "--diff-count",
    type=int,
    default=DEFAULT_DIFF_COUNT,
    help="Print the first N entries where the results do not match.",
    show_default=True,
)
@click.option(
    "--print-suggested-tolerances",
    is_flag=True,
    default=DEFAULT_PRINT_SUGGESTED_TOLERANCES,
    help=(
        "On failure, prints a set of potential tolerances based on the pareto"
        " frontier of passing absolute and relative tolerance combinations."
    ),
)
def main(
    pipeline_outputs: Path,
    torch_outputs: Path,
    eval_metric: CommaSeparatedList,
    relative_tolerance: float,
    absolute_tolerance: float,
    cos_dist_threshold: float,
    kl_div_threshold: float,
    diff_count: int,
    print_suggested_tolerances: bool,
) -> None:
    """Click command entry point that delegates to the implementation function.

    This wrapper exists because Click command functions aren't easily picklable,
    which causes issues when called from multiprocessing.
    """

    result = verify(
        pipeline_outputs=pipeline_outputs,
        torch_outputs=torch_outputs,
        eval_metric=eval_metric,
        relative_tolerance=relative_tolerance,
        absolute_tolerance=absolute_tolerance,
        cos_dist_threshold=cos_dist_threshold,
        kl_div_threshold=kl_div_threshold,
        diff_count=diff_count,
        print_suggested_tolerances=print_suggested_tolerances,
    )

    if not result.passed:
        assert result.error_message is not None
        raise AccuracyError(result.error_message)


@dataclass(frozen=True)
class DiscrepancyReport:
    """Contains discrepancy metrics between model outputs."""

    model_modality: ModelModality
    """Type of model output modality being compared."""

    mae_per_prompt: list[float]
    """Mean absolute error for each prompt."""

    rmse_per_prompt: list[float]
    """Root mean square error for each prompt."""

    kl_div_per_prompt: list[float] | None = None
    """KL divergence for each prompt (only for logit outputs)."""

    max_kl_div: float | None = None
    """Max KL divergence across all prompts (only for logit outputs)."""

    @property
    def avg_mae(self) -> float:
        """Calculate average mean absolute error across all prompts."""
        return sum(self.mae_per_prompt) / len(self.mae_per_prompt)

    @property
    def avg_rmse(self) -> float:
        """Calculate average root mean square error across all prompts."""
        return sum(self.rmse_per_prompt) / len(self.rmse_per_prompt)

    @property
    def avg_kl_div(self) -> float | None:
        """Calculate average KL divergence across all prompts (only for logit outputs)."""
        if self.kl_div_per_prompt is None:
            return None
        return sum(self.kl_div_per_prompt) / len(self.kl_div_per_prompt)

    @property
    def default_metric(self) -> float:
        """A default avg error metric for the type of model (KL Div for LLM)"""
        if self.model_modality == ModelModality.LOGIT:
            assert self.avg_kl_div is not None
            return self.avg_kl_div
        elif self.model_modality == ModelModality.EMBEDDING:
            return self.avg_mae
        else:
            raise ValueError(f"Unknown model modality: {self.model_modality}")


@dataclass(frozen=True)
class VerificationResult:
    """Result of model output verification."""

    passed: bool
    """Whether the verification passed or failed."""

    discrepancy_report: DiscrepancyReport
    """Report containing discrepancy metrics between model outputs."""

    error_message: str | None = None
    """Error message if verification failed, None otherwise."""


def verify(
    pipeline_outputs: Path,
    torch_outputs: Path,
    eval_metric: Sequence[str] | None = None,
    relative_tolerance: float | None = None,
    absolute_tolerance: float | None = None,
    cos_dist_threshold: float | None = None,
    kl_div_threshold: float | None = None,
    diff_count: int | None = None,
    print_suggested_tolerances: bool | None = None,
) -> VerificationResult:
    """Verify that pipeline outputs match torch outputs within specified tolerances.

    Args:
        pipeline_outputs: Path to the pipeline outputs JSON file
        torch_outputs: Path to the torch outputs JSON file
        eval_metric: Metrics to use for evaluation (e.g., ["tol", "cos", "kl"])
        relative_tolerance: Relative tolerance for numerical comparison
        absolute_tolerance: Absolute tolerance for numerical comparison
        cos_dist_threshold: Threshold for cosine similarity
        kl_div_threshold: Threshold for KL divergence
        diff_count: Number of differences to show in error output
        print_suggested_tolerances: Whether to print suggested tolerances on failure

    Returns:
        VerificationResult containing pass/fail status and discrepancy report
    """

    # MyPy needs the TypeVar in order to infer the type of the return value
    T = TypeVar("T")

    def val_or(value: T | None, default: T) -> T:
        return value if value is not None else default

    # Note: These default value shenanigans are here to simplify the logic
    # of the caller in generate_llm_logits.py.
    eval_metric = val_or(eval_metric, DEFAULT_EVAL_METRIC)
    relative_tolerance = val_or(relative_tolerance, DEFAULT_RELATIVE_TOLERANCE)
    absolute_tolerance = val_or(absolute_tolerance, DEFAULT_ABSOLUTE_TOLERANCE)
    cos_dist_threshold = val_or(cos_dist_threshold, DEFAULT_COS_DIST_THRESHOLD)
    kl_div_threshold = val_or(kl_div_threshold, DEFAULT_KL_DIV_THRESHOLD)
    diff_count = val_or(diff_count, DEFAULT_DIFF_COUNT)
    print_suggested_tolerances = val_or(
        print_suggested_tolerances, DEFAULT_PRINT_SUGGESTED_TOLERANCES
    )

    first_framework = "Pipeline"
    other_framework = "Torch"
    validator = construct_validator(
        eval_metric,
        atol=absolute_tolerance,
        rtol=relative_tolerance,
        cos_threshold=cos_dist_threshold,
        kl_div_threshold=kl_div_threshold,
    )

    pipeline_results: list[ModelOutput] = NumpyDecoder().decode(
        pipeline_outputs.read_text()
    )
    torch_results: list[ModelOutput] = NumpyDecoder().decode(
        torch_outputs.read_text()
    )

    pipeline_results, torch_results = normalize_logit_outputs(
        pipeline_results, torch_results
    )

    results = []
    any_failed = False

    def compare(
        pipeline_value: Any, torch_value: Any, description: str
    ) -> None:
        nonlocal any_failed

        # TODO: These assertions assume we're working with LLMs
        # or embedding models. They might need to be updated
        # when we add support for other model types.

        if isinstance(pipeline_value, int):
            # Integer fields (e.g., token ids, positions) are not compared for accuracy,
            # as they are either metadata or should be verified separately.
            return

        # For floats, we expect next_token_logits and check with tolerances
        if isinstance(pipeline_value, float):
            assert isinstance(torch_value, float)
            assert description.startswith("'next_token_logits'")
            if not math.isclose(
                pipeline_value,
                torch_value,
                rel_tol=2e-2,
                abs_tol=1e-6,
            ):
                print(
                    f"Got mismatching {description}: {pipeline_value} !="
                    f" {torch_value}"
                )
            return

        # For arrays, apply tolerance checking
        validation_result = validator(pipeline_value, torch_value)
        results.append(validation_result)
        if validation_result.any_failed():
            CONSOLE.print(
                f"===Got an error for the computed {description}."
                f"\n{validation_result.get_failure_messages()}"
            )
            any_failed = True

    compare_values(pipeline_results, torch_results, compare_fn=compare)

    report = compute_discrepancy_report(pipeline_results, torch_results)
    print_discrepancy_report(report)

    error_message = None
    test_passed = True
    if any_failed:
        test_passed = False
        validator.print_error_table(
            first_framework,
            other_framework,
            results,
            diff_count=diff_count,
            print_suggested_tolerances=print_suggested_tolerances,
        )

        if diff_count == 0:
            CONSOLE.print(
                "Use the --diff_count=N option to get more details on"
                " where the results differ."
            )

        error_message = (
            f'ðŸ‘Ž The outputs when using the "{first_framework}"'
            " framework is NOT within tolerance to the"
            f' outputs when using the "{other_framework}"'
            " framework."
        )
        CONSOLE.print(error_message)
    else:
        # the results are within tolerance, so we log a good message
        CONSOLE.print(
            f'ðŸ‘ The outputs when using the "{first_framework}" framework'
            f' are close to the outputs when using the "{other_framework}"'
            " framework for specified tolerances"
            f" ({validator.threshold_str()})"
        )

    return VerificationResult(
        passed=test_passed,
        discrepancy_report=report,
        error_message=error_message,
    )


def normalize_logit_outputs(
    pipeline_results: list[ModelOutput],
    torch_results: list[ModelOutput],
) -> tuple[list[ModelOutput], list[ModelOutput]]:
    """Normalize logit outputs to a consistent format for comparison.

    Different frameworks (MAX, PyTorch, vLLM) may output logits or logprobs.
    This function detects the output mode and converts both result sets to
    a common format with explicit logits arrays, enabling apples-to-apples
    comparison.

    Returns the inputs unchanged if neither contains token values.
    """
    all_outputs = pipeline_results + torch_results
    if not any("values" in output for output in all_outputs):
        return pipeline_results, torch_results

    modes = [
        ModelOutputView(output).mode
        for output in all_outputs
        if "values" in output
    ]
    mode = "logprobs" if "logprobs" in modes else "logits"

    def normalize(output: ModelOutput) -> ModelOutput:
        if "values" not in output:
            return output
        view = ModelOutputView(output)
        arrays = view.logits if mode == "logits" else view.logprobs
        return {
            "prompt": view.prompt,
            "values": [
                {
                    "next_token": token_info["next_token"],
                    "next_token_logits": float(arr[token_info["next_token"]]),
                    "logits": arr,
                }
                for token_info, arr in zip(view.values, arrays, strict=True)
            ],
        }

    return (
        [normalize(output) for output in pipeline_results],
        [normalize(output) for output in torch_results],
    )


def compute_discrepancy_report(
    results: list[ModelOutput], references: list[ModelOutput]
) -> DiscrepancyReport:
    """Calculate discrepancy metrics between outputs from two model runs.

    Args:
        results: List of model outputs to evaluate
        references: List of reference model outputs to compare against

    Returns:
        A DiscrepancyReport containing the calculated metrics
    """
    if len(results) != len(references):
        raise ValueError("The two lists must have the same length")

    mae_per_prompt, rmse_per_prompt, kl_div_per_prompt = [], [], []
    model_modality: ModelModality | None = None
    model_max_kl_div: float | None = None
    for result, reference in zip(results, references, strict=True):
        verify_matching_prompts(result, reference)
        avg_kl_div = None
        max_kl_div = None
        if "embeddings" in result and "embeddings" in reference:
            mae, rmse = calculate_mae_and_rmse(
                result["embeddings"].astype(np.float64),
                reference["embeddings"].astype(np.float64),
            )
            model_modality = ModelModality.EMBEDDING
        elif "values" in result and "values" in reference:
            mae, rmse, avg_kl_div, max_kl_div = calculate_logit_discrepancies(
                result["values"], reference["values"]
            )
            model_modality = ModelModality.LOGIT
        else:
            raise ValueError(
                "Unknown model modality, did not find embeddings or values"
            )
        mae_per_prompt.append(mae)
        rmse_per_prompt.append(rmse)
        if avg_kl_div is not None:
            kl_div_per_prompt.append(avg_kl_div)
        if max_kl_div is not None:
            if model_max_kl_div is None:
                model_max_kl_div = max_kl_div
            else:
                model_max_kl_div = max(model_max_kl_div, max_kl_div)

    if model_modality is None:
        raise ValueError("Could not determine model modality")

    return DiscrepancyReport(
        mae_per_prompt=mae_per_prompt,
        rmse_per_prompt=rmse_per_prompt,
        kl_div_per_prompt=kl_div_per_prompt if kl_div_per_prompt else None,
        max_kl_div=model_max_kl_div,
        model_modality=model_modality,
    )


def verify_matching_prompts(
    result: ModelOutput, reference: ModelOutput
) -> None:
    """Verify that the prompts match between result and reference.

    Args:
        result: Model output from the model being evaluated
        reference: Model output from the reference model

    Returns:
        None
    """
    if result["prompt"] != reference["prompt"]:
        raise ValueError(
            f"Mismatched prompts:\nResult: {result['prompt']}\nReference: {reference['prompt']}"
        )


def calculate_mae_and_rmse(
    result: np.ndarray, reference: np.ndarray
) -> tuple[float, float]:
    """Calculate MAE and RMSE between result and reference embeddings.

    Args:
        result: Vector of floats
        reference: Reference vector of floats

    Returns:
        A tuple containing (MAE, RMSE) as float values
    """
    diff = result - reference
    mae = np.mean(np.abs(diff))
    rmse = np.sqrt(np.mean(diff**2))
    return float(mae), float(rmse)


def calculate_logit_discrepancies(
    result_values: list[TokenInfo], reference_values: list[TokenInfo]
) -> tuple[float, float, float, float]:
    """Calculate MAE, RMSE and KL Divergence between result and reference logits.

    Args:
        result_values: List of token logits from the model being evaluated
        reference_values: List of token logits from the reference model

    Returns:
        A tuple containing (MAE, RMSE, avg KL_divergence, max KL_divergence)
    """
    total_mae = 0.0
    total_rmse = 0.0
    total_kl_div = 0.0
    max_kl_div = 0.0
    steps = 0

    for res_token, ref_token in zip(
        result_values, reference_values, strict=True
    ):
        res_logits_float64 = res_token["logits"].astype(np.float64)
        ref_logits_float64 = ref_token["logits"].astype(np.float64)
        mae, rmse = calculate_mae_and_rmse(
            res_logits_float64, ref_logits_float64
        )

        total_mae += mae
        total_rmse += rmse
        total_kl_div += kl_divergence_from_logits(
            res_logits_float64, ref_logits_float64
        )
        max_kl_div = max(
            max_kl_div,
            kl_divergence_from_logits(res_logits_float64, ref_logits_float64),
        )
        steps += 1

    mae_average = total_mae / steps
    rmse_average = total_rmse / steps
    kl_div_average = total_kl_div / steps

    return (
        float(mae_average),
        float(rmse_average),
        float(kl_div_average),
        float(max_kl_div),
    )


def print_discrepancy_report(report: DiscrepancyReport) -> None:
    """Print discrepancy metrics in a standardized format.

    Args:
        report: DiscrepancyReport containing the metrics to print
    """
    # Format the per-prompt values
    formatter = "{:.2e}".format
    formatted_mae = ", ".join(
        formatter(error) for error in report.mae_per_prompt
    )
    formatted_rmse = ", ".join(
        formatter(error) for error in report.rmse_per_prompt
    )

    CONSOLE.print(
        f"\n===== {report.model_modality.value} discrepancy report =====\n"
    )

    if report.kl_div_per_prompt is not None:
        formatted_kl_div = ", ".join(
            formatter(error) for error in report.kl_div_per_prompt
        )
        CONSOLE.print(f"KL Div: {formatter(report.max_kl_div)}")

    CONSOLE.print(f"RMSE:   {formatter(report.avg_rmse)}")
    CONSOLE.print(f"MAE:    {formatter(report.avg_mae)}")
    CONSOLE.print("\nPer prompt discrepancy numbers:")

    if report.kl_div_per_prompt is not None:
        CONSOLE.print(f"KL Div: [{formatted_kl_div}]")

    CONSOLE.print(f"RMSE:   [{formatted_rmse}]")
    CONSOLE.print(f"MAE:    [{formatted_mae}]")

    CONSOLE.print("\nLower numbers are better.")
    CONSOLE.print(
        f"They measure how close the {report.model_modality}s are between the two frameworks."
    )

    CONSOLE.print("\n===== end discrepancy report =====\n")


if __name__ == "__main__":
    main()
