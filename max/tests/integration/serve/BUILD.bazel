load("//bazel:api.bzl", "modular_py_test", "requirement")

modular_py_test(
    name = "tests_cpu_pure",
    size = "large",
    srcs = [
        "__init__.py",
        "conftest.py",
        "test_sagemaker_cpu.py",
        "test_stop_cpu.py",
    ],
    data = [
        "//SDK/integration-test/pipelines/python/llama3/testdata",
        "@test_llama_golden",
    ],
    env = {
        "PIPELINES_TESTDATA": "SDK/integration-test/pipelines/python/llama3/testdata",
    },
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/serve",
        "//SDK/lib/API/python/max/serve/schemas",
        requirement("async-asgi-testclient"),
        requirement("pytest-asyncio"),
        requirement("sse-starlette"),
    ],
)

modular_py_test(
    name = "tests_cpu_hf",
    size = "large",
    srcs = [
        "__init__.py",
        "conftest.py",
        "test_embeddings_cpu.py",
        "test_metrics_e2e_cpu.py",
        "test_models_api_cpu.py",
        "test_tinyllama_serving_cpu.py",
    ],
    data = [
        "//SDK/integration-test/pipelines/python/llama3/testdata",
        "@test_llama_golden",
    ],
    env = {
        "PIPELINES_TESTDATA": "SDK/integration-test/pipelines/python/llama3/testdata",
    },
    tags = [
        "AITLIB-293",
        "manual",  # TODO(AITLIB-293)
        "no-sandbox",
        "requires-network",
    ],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/serve",
        "//SDK/lib/API/python/max/serve/schemas",
        requirement("async-asgi-testclient"),
        requirement("pytest-asyncio"),
        requirement("sse-starlette"),
    ],
)

modular_py_test(
    name = "tests_gpu",
    size = "enormous",
    srcs = [
        "__init__.py",
        "conftest.py",
        "test_smollm_serving_gpu.py",
    ],
    data = [
        "//Kernels/tools/gpu-query",
        "//SDK/integration-test/pipelines/python/llama3/testdata",
        "@test_llama_golden",
    ],
    env = {
        "PIPELINES_TESTDATA": "SDK/integration-test/pipelines/python/llama3/testdata",
        "MODULAR_CUDA_QUERY_PATH": "$(location //Kernels/tools/gpu-query)",
    },
    exec_properties = {
        # Ensure the test has at least 23GB of GPU memory available.
        "resources:gpu-memory": "23",
    },
    gpu_constraints = [
        "//:has_gpu",
        "//:nvidia_gpu",
    ],
    tags = [
        "gpu",
        "no-sandbox",
        "requires-network",
    ],
    deps = [
        "//SDK/integration-test/pipelines/python/test_common",
        "//SDK/lib/API/python/max/engine",
        "//SDK/lib/API/python/max/pipelines/architectures",
        "//SDK/lib/API/python/max/serve",
        requirement("async-asgi-testclient"),
        requirement("pytest-asyncio"),
        requirement("sse-starlette"),
        requirement("transformers"),
    ],
)
