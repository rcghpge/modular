# MAX framework

MAX is a high-performance inference server that provides
an [OpenAI-compatible endpoint](https://docs.modular.com/max/api/serve) for
large language models (LLMs) and it's a fundamental component of the
[Modular Platform](https://docs.modular.com/max/intro).

This directory includes the source for our Python-based inference server,
Python-based model pipelines (graphs), Python-based neural-net operators
(high-level graph ops), Mojo-based kernel functions (low-level graph
ops for GPUs and CPUs), and more.

## Usage

With just a few commands, you can use MAX to create a local endpoint serving a
large language model (LLM) of your choice, using our CLI tool or Docker
container. Try it now with our [quickstart
guide](https://docs.modular.com/max/get-started).

See [https://builds.modular.com/](https://builds.modular.com/) to discover many
of the models supported by MAX.

## Contributing

Thanks for your interest in contributing to MAX!

We welcome contributions to this repo on the
[`main`](https://github.com/modular/modular/tree/main)
branch. Please first read our [Contributor
Guide](https://github.com/modular/modular/blob/main/max/CONTRIBUTING.md).

If you want to report issues or request features, [please create a GitHub
issue here](https://github.com/modular/modular/issues)â€”also see our [guide to
submitting good bug reports](./CONTRIBUTING.md#submitting-bugs).

## Contact us

If you'd like to chat with the team and other community members, please send a
message to our [Discord channel](https://discord.gg/modular) and [our
forum board](https://forum.modular.com/).
