:description: Legacy attention mechanisms for graph-based models.
:title: attention
:type: package
:lang: python
:wrapper_class: rst-index
:source: max/nn/legacy/attention/

max.nn.legacy.attention
-----------------------

Legacy attention mechanisms for graph-based neural networks.

Modules
=======

* :code_link:`/max/api/python/nn/legacy/attention/attention_with_rope|attention_with_rope`: Attention with rotary position embeddings.
* :code_link:`/max/api/python/nn/legacy/attention/interfaces|interfaces`: Attention interface definitions.
* :code_link:`/max/api/python/nn/legacy/attention/mask_config|mask_config`: Attention mask configuration utilities.
* :code_link:`/max/api/python/nn/legacy/attention/multi_latent_attention|multi_latent_attention`: Multi-latent attention mechanism.
* :code_link:`/max/api/python/nn/legacy/attention/multihead_attention|multihead_attention`: Multi-head attention implementation.
* :code_link:`/max/api/python/nn/legacy/attention/ragged_attention|ragged_attention`: Attention for variable-length sequences.


.. toctree::
   :hidden:

   interfaces
   attention_with_rope
   mask_config
   multi_latent_attention
   multihead_attention
   ragged_attention
