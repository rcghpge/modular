##===----------------------------------------------------------------------===##
# Copyright (c) 2026, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##===----------------------------------------------------------------------===##

# Serving Benchmark Configuration
# This configuration inherits from base_config.yaml and adds serving-specific parameters.
# Used by benchmark_serving.py and other serving benchmarks.

# Configuration metadata
name: "Serving Benchmark Configuration"
description: "Configuration for online serving benchmarks (benchmark_serving.py)"
version: "0.0.1"

# Inherit from base configuration
depends_on: "base_config.yaml"

# Serving-specific benchmark parameters
benchmark_config:
  # Backend and API configuration (serving-specific)
  backend: "modular"  # choices: vllm, vllm-chat, modular, modular-chat, sglang, sglang-chat
  base_url: null  # Server or API base url if not using host/port
  host: "localhost"
  port: 8000
  endpoint: "/v1/chat/completions"  # /v1/completions, /v1/chat/completions, /v2/models/ensemble/generate_stream

  num_prompts: 1000  # Number of prompts to process (serving-specific)

  # Dataset configuration (serving-specific)
  dataset_name: "sharegpt"  # has to be set to a valid dataset name for serving benchmarks

  # Request configuration (serving-specific)
  max_concurrency: null  # Maximum concurrent requests (optimized for serving benchmarks)
  lora: null  # Optional LoRA name

  # Workload configuration (serving-specific)
  max_benchmark_duration_s: null  # Maximum benchmark duration in seconds
  num_chat_sessions: null  # Number of multiturn chat sessions
  delay_between_chat_turns: null  # Delay between chat turns in ms

  # Output control (serving-specific extensions)
  output_lengths: null  # Path to YAML file with output lengths or int
  max_output_len: null  # Maximum output length per request
  temperature: null  # Temperature for sampling
  top_p: null  # Top-p for sampling
  top_k: null  # Top-k for sampling

  # Traffic control (serving-specific)
  request_rate: "inf"  # Requests per second (finite rate for realistic benchmarking)
  burstiness: 1.0  # Burstiness factor (1.0 = Poisson process)
  skip_first_n_requests: 0  # Skip first N requests for measurements
  chat_warmup_delay_ms: 0.0  # Delay between starting chat sessions
  ignore_first_turn_stats: false # Ignore the first turn in a multi-turn chat.

  # Dataset-specific parameters (serving workloads)
  arxiv_summarization_input_len: 15000
  obfuscated_conversations_average_output_len: 175
  obfuscated_conversations_coefficient_of_variation: 0.1
  obfuscated_conversations_shuffle: false
  random_coefficient_of_variation: "0.3,0.7"
  random_distribution_type: "normal"  # choices: uniform, normal
  random_first_turn_ratio: 1.0
  random_image_count: 0
  random_image_size: ""
  random_input_len: 1024
  random_max_num_unique_sys_prompt: 1
  random_num_turns: 1
  random_output_len: 128
  random_sys_prompt_ratio: 0.0
  sonnet_input_len: 550
  sonnet_prefix_len: 200

  # Control flags (serving-specific)
  skip_test_prompt: false
  collect_gpu_stats: false  # Enable GPU stats collection for serving benchmarks
  collect_cpu_stats: true  # Enable CPU stats collection for serving benchmarks
  collect_server_stats: true  # Enable server stats collection for serving benchmarks
  trace: false  # Enable tracing of the benchmark run (currently NVIDIA GPUs only)
  trace_file: null  # Path to save trace file
  trace_session: null  # Optional session name to trace. If not specified, nsys traces the default session.

  # Result saving (serving-specific extensions)
  record_output_lengths: null  # Path to save output lengths in YAML format
  result_filename: null  # JSON filename for results. If null, no results are saved. Can include directory path.
  metadata: []  # Key-value pairs for metadata (format: ["key=value", ...])

  # LoRA-specific parameters (serving workloads)
  lora_paths: []
  lora_uniform_traffic_ratio: 0.0
  per_lora_traffic_ratio: []
  max_concurrent_lora_ops: 1

  # Batch job dataset parameters
  batch_job_image_dir: null  # When set, use file references instead of embedded images
