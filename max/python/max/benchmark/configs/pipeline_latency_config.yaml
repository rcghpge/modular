##===----------------------------------------------------------------------===##
# Copyright (c) 2025, Modular Inc. All rights reserved.
#
# Licensed under the Apache License v2.0 with LLVM Exceptions:
# https://llvm.org/LICENSE.txt
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
##===----------------------------------------------------------------------===##

# Pipeline Latency Benchmark Configuration
# This configuration inherits from base_config.yaml and adds pipeline latency-specific parameters.
# Used by benchmark_pipeline_latency.py for synthetic batched pipeline performance benchmarks.

# Configuration metadata
name: "Pipeline Latency Benchmark Configuration"
description: "Configuration for pipeline latency benchmarks (benchmark_pipeline_latency.py)"
version: "0.0.1"

# Inherit from base configuration.
depends_on: "base_config.yaml"

# Pipeline latency-specific benchmark parameters
benchmark_config:
  # Model configuration (pipeline latency-specific)
  weight_path: null  # Override the default weights file. Must be in GGUF format.
  lora_paths: null  # Comma-separated list of paths to LoRA adapters

  # Shape options (pipeline latency-specific)
  batch_size: 1  # Number of prompts in each batch
  ce_batch_size: null  # Number of prompts in each batch during context-encoding
  input_len: 256  # Number of input tokens in each prompt
  input_image_size: null  # Size of input images in pixels (WxH format)
  output_len: 128  # Number of output tokens generated per prompt
  max_len: null  # Maximum total length of text sequences. Use 'auto' for max-len=max_input_len+max_output_len
  shape_file: null  # YAML file containing list of shapes in the batch
  chunked_prefill_size: null  # Enable chunked prefill with given chunk size

  # Hardware options (pipeline latency-specific)
  devices: null  # Hardware device on which model will be executed. Valid values: 'cpu', 'gpu', 'gpu:0,1,2'
  quantization_encoding: "q4_k"  # Quantization encoding to benchmark

  # Execution options (pipeline latency-specific)
  num_warmups: 1  # Number of warmup iterations to run
  num_iters: 1  # Number of benchmarking iterations to run
  enable_prefix_caching: true  # Enable prefix caching for paged-attention kvcache
  num_scheduler_steps: 1  # Number of steps to generate per Token Gen iteration
  cache_strategy: "paged"  # The KVCache implementation to use. Choices: model_default, paged

  # Sampling options (pipeline latency-specific)
  top_k: 1  # Limits the sampling to the K most probable tokens. Default: 1 (greedy sampling)

  # Profiling options (pipeline latency-specific)
  gpu_sampling_interval: null  # Interval, in seconds, between GPU resource samples
  print_gpu_utilization_table: null  # Print statistics for each individual GPU in the system. Choices: csv, pretty
  print_ce_chunk_latencies: false  # Print latencies per ce chunk
  print_tg_token_latencies: false  # Print latencies per output token
